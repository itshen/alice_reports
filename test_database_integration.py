#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“é›†æˆæµ‹è¯• - éªŒè¯æœç´¢ç»“æœå…¥åº“åŠŸèƒ½
"""

import asyncio
import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app import app, db
from models import GlobalSettings, CrawlerConfig, CrawlRecord
from services.deep_research_service import DeepResearchService
from services.crawler_service import CrawlerService
from services.llm_service import LLMService

async def test_database_integration():
    """æµ‹è¯•æ•°æ®åº“é›†æˆåŠŸèƒ½"""
    print("ğŸ—„ï¸ æµ‹è¯•æ·±åº¦ç ”ç©¶æ•°æ®åº“é›†æˆåŠŸèƒ½")
    print("="*60)
    
    with app.app_context():
        # è·å–å…¨å±€è®¾ç½®
        settings = GlobalSettings.query.first()
        if not settings:
            print("âŒ æœªæ‰¾åˆ°å…¨å±€è®¾ç½®")
            return False
        
        # åˆ›å»ºæœåŠ¡å®ä¾‹
        crawler_service = CrawlerService()
        llm_service = LLMService()
        llm_service.update_settings(settings)
        deep_research_service = DeepResearchService(crawler_service, llm_service)
        
        print("âœ… æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        
        # 1. æ£€æŸ¥ç°æœ‰æ•°æ®
        print("\nğŸ“Š æ£€æŸ¥ç°æœ‰æ•°æ®åº“çŠ¶æ€...")
        
        total_crawlers = CrawlerConfig.query.count()
        total_records = CrawlRecord.query.count()
        deep_research_crawler = CrawlerConfig.query.filter_by(name="æ·±åº¦ç ”ç©¶").first()
        
        print(f"   æ€»çˆ¬è™«æ•°: {total_crawlers}")
        print(f"   æ€»è®°å½•æ•°: {total_records}")
        print(f"   æ·±åº¦ç ”ç©¶çˆ¬è™«: {'å·²å­˜åœ¨' if deep_research_crawler else 'ä¸å­˜åœ¨'}")
        
        if deep_research_crawler:
            deep_records = CrawlRecord.query.filter_by(
                crawler_config_id=deep_research_crawler.id
            ).count()
            print(f"   æ·±åº¦ç ”ç©¶è®°å½•æ•°: {deep_records}")
        
        # 2. æµ‹è¯•ä¿å­˜æœç´¢ç»“æœ
        print("\nğŸ’¾ æµ‹è¯•ä¿å­˜æœç´¢ç»“æœåˆ°æ•°æ®åº“...")
        
        # æ¨¡æ‹Ÿä¸€ä¸ªæœç´¢ç»“æœ
        mock_crawl_result = {
            'title': 'æµ‹è¯•æ·±åº¦ç ”ç©¶æ–‡ç«  - AIæŠ€æœ¯å‘å±•',
            'content': 'è¿™æ˜¯ä¸€ç¯‡å…³äºAIæŠ€æœ¯å‘å±•çš„æµ‹è¯•æ–‡ç« å†…å®¹ã€‚åŒ…å«äº†äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ ç­‰å…³é”®è¯ã€‚',
            'url': f'https://test-deep-research.com/article/{datetime.now().strftime("%Y%m%d%H%M%S")}',
            'author': 'æµ‹è¯•ä½œè€…',
            'success': True
        }
        
        try:
            await deep_research_service._save_search_result_to_db(mock_crawl_result, "AIæŠ€æœ¯å‘å±•")
            print("âœ… æœç´¢ç»“æœä¿å­˜æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æœç´¢ç»“æœä¿å­˜å¤±è´¥: {e}")
            return False
        
        # 3. éªŒè¯æ•°æ®æ˜¯å¦æ­£ç¡®ä¿å­˜
        print("\nğŸ” éªŒè¯ä¿å­˜çš„æ•°æ®...")
        
        # é‡æ–°æŸ¥è¯¢æ·±åº¦ç ”ç©¶çˆ¬è™«
        deep_research_crawler = CrawlerConfig.query.filter_by(name="æ·±åº¦ç ”ç©¶").first()
        if not deep_research_crawler:
            print("âŒ æ·±åº¦ç ”ç©¶çˆ¬è™«æœªåˆ›å»º")
            return False
        
        print(f"âœ… æ·±åº¦ç ”ç©¶çˆ¬è™«å·²åˆ›å»ºï¼ŒID: {deep_research_crawler.id}")
        
        # æŸ¥è¯¢åˆšä¿å­˜çš„è®°å½•
        saved_record = CrawlRecord.query.filter_by(
            url=mock_crawl_result['url']
        ).first()
        
        if not saved_record:
            print("âŒ ä¿å­˜çš„è®°å½•æœªæ‰¾åˆ°")
            return False
        
        print(f"âœ… è®°å½•ä¿å­˜æˆåŠŸ:")
        print(f"   æ ‡é¢˜: {saved_record.title}")
        print(f"   URL: {saved_record.url}")
        print(f"   å†…å®¹é•¿åº¦: {len(saved_record.content)} å­—ç¬¦")
        print(f"   çˆ¬è™«ID: {saved_record.crawler_config_id}")
        
        # 4. æµ‹è¯•ä»æ•°æ®åº“è·å–æ·±åº¦ç ”ç©¶æ•°æ®
        print("\nğŸ“š æµ‹è¯•ä»æ•°æ®åº“è·å–æ·±åº¦ç ”ç©¶å†å²æ•°æ®...")
        
        # åˆ›å»ºæµ‹è¯•æŠ¥å‘Šé…ç½®
        test_config = type('TestConfig', (), {
            'data_sources': '1,2',  # ä½¿ç”¨å‰ä¸¤ä¸ªçˆ¬è™«
            'filter_keywords': 'AI,äººå·¥æ™ºèƒ½,æŠ€æœ¯',
            'time_range': '7d'
        })()
        
        try:
            knowledge_base = await deep_research_service._build_initial_knowledge_base(test_config)
            print(f"âœ… çŸ¥è¯†åº“æ„å»ºæˆåŠŸï¼Œå…± {len(knowledge_base)} ç¯‡æ–‡ç« ")
            
            # ç»Ÿè®¡æ•°æ®æ¥æº
            sources = {}
            for article in knowledge_base:
                source = article.get('source', 'æœªçŸ¥æ¥æº')
                sources[source] = sources.get(source, 0) + 1
            
            print("ğŸ“Š æ•°æ®æ¥æºç»Ÿè®¡:")
            for source, count in sources.items():
                print(f"   {source}: {count} ç¯‡")
                
        except Exception as e:
            print(f"âŒ çŸ¥è¯†åº“æ„å»ºå¤±è´¥: {e}")
            return False
        
        # 5. æµ‹è¯•é‡å¤URLæ£€æŸ¥
        print("\nğŸ”„ æµ‹è¯•é‡å¤URLæ£€æŸ¥...")
        
        try:
            # å°è¯•ä¿å­˜ç›¸åŒURLçš„è®°å½•
            await deep_research_service._save_search_result_to_db(mock_crawl_result, "é‡å¤æµ‹è¯•")
            print("âœ… é‡å¤URLæ£€æŸ¥æ­£å¸¸ï¼ˆåº”è¯¥è·³è¿‡é‡å¤è®°å½•ï¼‰")
        except Exception as e:
            print(f"âŒ é‡å¤URLæ£€æŸ¥å¤±è´¥: {e}")
            return False
        
        print("\n" + "="*60)
        print("ğŸ‰ æ•°æ®åº“é›†æˆæµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        print("âœ… æœç´¢ç»“æœå¯ä»¥æ­£ç¡®å…¥åº“")
        print("âœ… æ·±åº¦ç ”ç©¶çˆ¬è™«è‡ªåŠ¨åˆ›å»º")
        print("âœ… å†å²æ•°æ®å¯ä»¥æ­£ç¡®è·å–")
        print("âœ… é‡å¤URLæ£€æŸ¥æ­£å¸¸å·¥ä½œ")
        print("="*60)
        
        return True

if __name__ == "__main__":
    success = asyncio.run(test_database_integration())
    
    if success:
        print("\nğŸŠ æ•°æ®åº“é›†æˆæµ‹è¯•å®Œæˆï¼")
        print("ğŸ’¡ ç°åœ¨æ·±åº¦ç ”ç©¶åŠŸèƒ½ä¼š:")
        print("   1. ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ä¸­çš„ç°æœ‰æ•°æ®")
        print("   2. å°†æœç´¢åˆ°çš„æ–°å†…å®¹è‡ªåŠ¨å…¥åº“")
        print("   3. é¿å…é‡å¤çˆ¬å–ç›¸åŒURL")
        sys.exit(0)
    else:
        print("\nğŸ’¥ æ•°æ®åº“é›†æˆæµ‹è¯•å¤±è´¥ï¼")
        sys.exit(1)
