#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±åº¦ç ”ç©¶æœåŠ¡ - V3.0 XMLäº¤äº’ç‰ˆ
å®ç°è¿­ä»£å¼AIç ”ç©¶æµç¨‹
"""

import asyncio
import logging
import xml.etree.ElementTree as ET
from typing import List, Dict, Any, Tuple
from datetime import datetime
import re

from services.notification_service import NotificationService

logger = logging.getLogger(__name__)

class DeepResearchService:
    """æ·±åº¦ç ”ç©¶æœåŠ¡ç±» - V3.0 XMLäº¤äº’ç‰ˆ"""
    
    def __init__(self, crawler_service, llm_service):
        self.crawler_service = crawler_service
        self.llm_service = llm_service
        self.notification_service = NotificationService()
        self.max_iterations = 30
        
    async def conduct_deep_research(self, report_config, settings) -> Dict[str, Any]:
        """æ‰§è¡Œæ·±åº¦ç ”ç©¶æµç¨‹"""
        try:
            logger.info(f"å¼€å§‹æ·±åº¦ç ”ç©¶: {report_config.name}")
            
            # 0. æ›´æ–°LLMè®¾ç½®
            self.llm_service.update_settings(settings)
            
            # 1. ä»æ•°æ®åº“è·å–æœ€è¿‘çˆ¬åˆ°çš„å†…å®¹
            initial_articles = await self._build_initial_knowledge_base(report_config)
            
            if not initial_articles:
                return {
                    'success': False,
                    'message': 'åˆå§‹æ•°æ®è·å–å¤±è´¥ï¼Œæœªä»æ•°æ®åº“ä¸­æ‰¾åˆ°ç›¸å…³æ•°æ®'
                }
            
            # 2. ä½¿ç”¨é…ç½®å…³é”®è¯è¿›è¡ŒåŸºç¡€è¿‡æ»¤ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if report_config.filter_keywords:
                filtered_articles = self.llm_service.filter_articles_by_keywords(
                    initial_articles, report_config.filter_keywords
                )
                logger.info(f"é…ç½®å…³é”®è¯è¿‡æ»¤: {len(initial_articles)} -> {len(filtered_articles)} ç¯‡æ–‡ç« ")
                knowledge_base = filtered_articles
            else:
                knowledge_base = initial_articles
            
            if not knowledge_base:
                return {
                    'success': False,
                    'message': 'è¿‡æ»¤åæœªæ‰¾åˆ°ç›¸å…³æ–‡ç« '
                }
            
            # 3. å¼€å§‹è¿­ä»£ç ”ç©¶ - AIåˆ¤æ–­ç°æœ‰èµ„æ–™æ˜¯å¦è¶³å¤Ÿå†™æŠ¥å‘Š
            iteration_count = 0
            research_log = []
            
            while iteration_count < self.max_iterations:
                iteration_count += 1
                logger.info(f"å¼€å§‹ç¬¬ {iteration_count} è½®ç ”ç©¶è¿­ä»£")
                
                # AIåˆ†æå½“å‰çŸ¥è¯†åº“ï¼Œåˆ¤æ–­æ˜¯å¦èƒ½å†™æŠ¥å‘Š
                ai_response = await self._send_research_prompt(
                    knowledge_base, 
                    report_config,
                    iteration_count
                )
                
                research_log.append({
                    'iteration': iteration_count,
                    'action': ai_response.get('action', 'unknown'),
                    'details': ai_response.get('details', ''),
                    'timestamp': datetime.now().isoformat()
                })
                
                if ai_response['action'] == 'finish':
                    logger.info(f"AIå†³å®šç»“æŸç ”ç©¶ï¼Œå…±è¿›è¡Œäº† {iteration_count} è½®è¿­ä»£")
                    break
                elif ai_response['action'] == 'search':
                    # æ‰§è¡ŒåŸºäºAIå†³ç­–çš„æœç´¢å’Œçˆ¬å–
                    keywords = ai_response.get('keywords', [])
                    if not keywords:
                        logger.warning(f"ç¬¬ {iteration_count} è½®ï¼šAIæœªæä¾›æœç´¢å…³é”®è¯")
                        break
                    
                    new_articles = await self._execute_search_and_crawl(
                        keywords, 
                        [], 
                        settings
                    )
                    
                    # æ‰©å……knowledge_base
                    knowledge_base.extend(new_articles)
                    logger.info(f"ç¬¬ {iteration_count} è½®: æ–°å¢ {len(new_articles)} ç¯‡æ–‡ç« åˆ°çŸ¥è¯†åº“")
                else:
                    logger.warning(f"AIè¿”å›äº†æœªçŸ¥çš„åŠ¨ä½œ: {ai_response['action']}")
                    break
            
            # 3. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_report = await self._generate_final_report(knowledge_base, report_config)
            
            # 4. æ¨é€æŠ¥å‘Šåˆ°é…ç½®çš„ç¾¤ç»„
            notification_sent = False
            if hasattr(report_config, 'webhook_url') and report_config.webhook_url:
                notification_sent = await self._send_report_notification(final_report, report_config)
            
            return {
                'success': True,
                'report': final_report,
                'knowledge_base_size': len(knowledge_base),
                'iterations': iteration_count,
                'research_log': research_log,
                'notification_sent': notification_sent
            }
            
        except Exception as e:
            logger.error(f"æ·±åº¦ç ”ç©¶å¤±è´¥: {e}")
            return {
                'success': False,
                'message': f'æ·±åº¦ç ”ç©¶å¤±è´¥: {str(e)}'
            }
    
    async def _build_initial_knowledge_base(self, report_config) -> List[Dict]:
        """æ„å»ºåˆå§‹çŸ¥è¯†åº“"""
        logger.info("æ„å»ºåˆå§‹çŸ¥è¯†åº“...")
        
        try:
            # è®¾ç½®æ•´ä¸ªæ–¹æ³•çš„è¶…æ—¶æ—¶é—´ä¸º30ç§’
            return await asyncio.wait_for(
                self._build_initial_knowledge_base_impl(report_config),
                timeout=30.0
            )
        except asyncio.TimeoutError:
            logger.error("æ„å»ºåˆå§‹çŸ¥è¯†åº“è¶…æ—¶ï¼ˆ30ç§’ï¼‰ï¼Œè¿”å›ç©ºç»“æœ")
            return []
        except Exception as e:
            logger.error(f"æ„å»ºåˆå§‹çŸ¥è¯†åº“å¤±è´¥: {e}")
            return []
    
    async def _build_initial_knowledge_base_impl(self, report_config) -> List[Dict]:
        """æ„å»ºåˆå§‹çŸ¥è¯†åº“çš„å…·ä½“å®ç°"""
        knowledge_base = []
        
        # è·å–æ•°æ®æºçˆ¬è™«ID
        if not report_config.data_sources:
            return knowledge_base
        
        crawler_ids = [int(x) for x in report_config.data_sources.split(',') if x.strip()]
        
        # å¯¼å…¥éœ€è¦åœ¨å‡½æ•°å†…éƒ¨è¿›è¡Œ
        from models import CrawlerConfig, CrawlRecord
        from datetime import timedelta
        
        # ä»æ¯ä¸ªçˆ¬è™«è·å–æ–‡ç« ï¼ˆå¢åŠ å•ä¸ªçˆ¬è™«å¤„ç†è¶…æ—¶ï¼‰
        for crawler_id in crawler_ids:
            try:
                # ä¸ºæ¯ä¸ªçˆ¬è™«è®¾ç½®5ç§’è¶…æ—¶
                await asyncio.wait_for(
                    self._process_single_crawler(crawler_id, report_config, knowledge_base),
                    timeout=5.0
                )
            except asyncio.TimeoutError:
                logger.error(f"å¤„ç†çˆ¬è™«ID {crawler_id} è¶…æ—¶ï¼ˆ5ç§’ï¼‰ï¼Œè·³è¿‡")
                continue
            except Exception as e:
                logger.error(f"å¤„ç†çˆ¬è™«ID {crawler_id} å¤±è´¥: {e}")
                continue
        
        # åŒæ—¶ä»"æ·±åº¦ç ”ç©¶"çˆ¬è™«è·å–å†å²æœç´¢æ•°æ®ï¼ˆå¢åŠ è¶…æ—¶ä¿æŠ¤ï¼‰
        try:
            await asyncio.wait_for(
                self._get_deep_research_history(report_config, knowledge_base),
                timeout=5.0
            )
        except asyncio.TimeoutError:
            logger.error("è·å–æ·±åº¦ç ”ç©¶å†å²æ•°æ®è¶…æ—¶ï¼ˆ5ç§’ï¼‰ï¼Œè·³è¿‡")
        except Exception as e:
            logger.error(f"è·å–æ·±åº¦ç ”ç©¶å†å²æ•°æ®å¤±è´¥: {e}")
        
        # åº”ç”¨å…³é”®è¯è¿‡æ»¤
        if report_config.filter_keywords:
            logger.info(f"å¼€å§‹å…³é”®è¯è¿‡æ»¤ï¼Œå½“å‰æ–‡ç« æ•°: {len(knowledge_base)}")
            logger.info(f"è¿‡æ»¤å…³é”®è¯: {report_config.filter_keywords}")
            
            filtered_kb = self.llm_service.filter_articles_by_keywords(
                knowledge_base, 
                report_config.filter_keywords
            )
            logger.info(f"å…³é”®è¯è¿‡æ»¤å®Œæˆ: {len(knowledge_base)} -> {len(filtered_kb)} ç¯‡æ–‡ç« ")
            knowledge_base = filtered_kb
        
        logger.info(f"åˆå§‹çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼Œå…± {len(knowledge_base)} ç¯‡æ–‡ç« ")
        return knowledge_base
    
    async def _process_single_crawler(self, crawler_id: int, report_config, knowledge_base: List[Dict]):
        """å¤„ç†å•ä¸ªçˆ¬è™«çš„æ•°æ®è·å–"""
        from models import CrawlerConfig, CrawlRecord
        from datetime import timedelta
        
        logger.info(f"æ­£åœ¨å¤„ç†çˆ¬è™«ID: {crawler_id}")
        crawler = CrawlerConfig.query.get(crawler_id)
        if not crawler:
            logger.warning(f"æœªæ‰¾åˆ°çˆ¬è™«ID: {crawler_id}")
            return
        
        logger.info(f"ä»çˆ¬è™« '{crawler.name}' è·å–æ•°æ®...")
        
        # å…ˆå°è¯•ä»å†å²è®°å½•è·å–
        time_range_hours = self._parse_time_range(report_config.time_range)
        cutoff_time = datetime.utcnow() - timedelta(hours=time_range_hours)
        
        logger.info(f"æŸ¥è¯¢æ—¶é—´èŒƒå›´: {time_range_hours}å°æ—¶ï¼Œæˆªæ­¢æ—¶é—´: {cutoff_time}")
        
        records = CrawlRecord.query.filter(
            CrawlRecord.crawler_config_id == crawler_id,
            CrawlRecord.status == 'success',
            CrawlRecord.crawled_at >= cutoff_time
        ).order_by(
            CrawlRecord.publish_date.desc().nulls_last(),
            CrawlRecord.crawled_at.desc()
        ).limit(10).all()
        
        if records:
            logger.info(f"ä»å†å²è®°å½•è·å– {len(records)} ç¯‡æ–‡ç« ")
            for record in records:
                knowledge_base.append({
                    'title': record.title,
                    'content': record.content,
                    'url': record.url,
                    'author': record.author,
                    'date': record.publish_date.isoformat() if record.publish_date else '',
                    'source': f'çˆ¬è™«: {crawler.name}',
                    'crawled_at': record.crawled_at.isoformat()
                })
        else:
            # å¦‚æœæ²¡æœ‰å†å²è®°å½•ï¼Œè·³è¿‡å®æ—¶çˆ¬å–ï¼Œåªä½¿ç”¨ç°æœ‰æ•°æ®
            logger.info(f"çˆ¬è™« '{crawler.name}' æš‚æ— ç¬¦åˆæ—¶é—´èŒƒå›´çš„å†å²æ•°æ®ï¼Œè·³è¿‡")
    
    async def _get_deep_research_history(self, report_config, knowledge_base: List[Dict]):
        """è·å–æ·±åº¦ç ”ç©¶å†å²æ•°æ®"""
        from models import CrawlerConfig, CrawlRecord
        from datetime import timedelta
        
        logger.info("è·å–æ·±åº¦ç ”ç©¶å†å²æ•°æ®...")
        deep_research_crawler = CrawlerConfig.query.filter_by(name="æ·±åº¦ç ”ç©¶").first()
        if deep_research_crawler:
            time_range_hours = self._parse_time_range(report_config.time_range)
            cutoff_time = datetime.utcnow() - timedelta(hours=time_range_hours)
            
            search_records = CrawlRecord.query.filter(
                CrawlRecord.crawler_config_id == deep_research_crawler.id,
                CrawlRecord.status == 'success',
                CrawlRecord.crawled_at >= cutoff_time
            ).order_by(
                CrawlRecord.publish_date.desc().nulls_last(),
                CrawlRecord.crawled_at.desc()
            ).limit(20).all()
            
            if search_records:
                logger.info(f"ä»æ·±åº¦ç ”ç©¶å†å²æ•°æ®è·å– {len(search_records)} ç¯‡æ–‡ç« ")
                for record in search_records:
                    knowledge_base.append({
                        'title': record.title,
                        'content': record.content,
                        'url': record.url,
                        'author': record.author,
                        'date': record.publish_date.isoformat() if record.publish_date else '',
                        'source': 'æ·±åº¦ç ”ç©¶å†å²æ•°æ®',
                        'crawled_at': record.crawled_at.isoformat()
                    })
            else:
                logger.info("æ·±åº¦ç ”ç©¶çˆ¬è™«æš‚æ— ç¬¦åˆæ—¶é—´èŒƒå›´çš„å†å²æ•°æ®")
        else:
            logger.warning("æœªæ‰¾åˆ°æ·±åº¦ç ”ç©¶çˆ¬è™«é…ç½®")
    
    async def _send_research_prompt(self, knowledge_base: List[Dict], report_config, iteration: int) -> Dict[str, Any]:
        """å‘é€ç ”ç©¶æç¤ºç»™AIï¼Œè§£æXMLå“åº”"""
        
        # æ„å»ºknowledge_base XML
        kb_xml = self._build_knowledge_base_xml(knowledge_base)
        
        # æ„å»ºuser_prompt XML
        user_prompt_xml = f"""<user_prompt>
<report_purpose>{report_config.purpose}</report_purpose>
<research_focus>{report_config.research_focus}</research_focus>
<filter_keywords>{report_config.filter_keywords}</filter_keywords>
</user_prompt>"""
        
        messages = [
            {
                "role": "system",
                "content": f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆï¼Œéœ€è¦åˆ¤æ–­ç°æœ‰èµ„æ–™æ˜¯å¦è¶³å¤Ÿå†™å‡ºä¸€ä»½åŸºäºå…·ä½“ç°è±¡çš„æ·±åº¦åˆ†ææŠ¥å‘Šã€‚

**æŠ¥å‘Šç›®æ ‡**: {report_config.purpose}

**åˆ¤æ–­æ ‡å‡†**ï¼š
1. **å¯ä»¥å†™æŠ¥å‘Šçš„æƒ…å†µ**ï¼š
   - æœ‰å…·ä½“çš„æ–°é—»äº‹ä»¶ã€å…¬å¸åŠ¨æ€ã€äº§å“å‘å¸ƒç­‰
   - æœ‰è¶³å¤Ÿçš„ç»†èŠ‚å’Œæ•°æ®æ”¯æ’‘æ·±åº¦åˆ†æ  
   - èƒ½å¤ŸåŸºäºæŸä¸ªå…·ä½“ç°è±¡è¿›è¡Œå¤šè§’åº¦è§£è¯»
   - å†…å®¹è¶³å¤Ÿå†™å‡ºäº‹ä»¶æ¦‚è¿°ã€æ·±åº¦åˆ†æã€å½±å“è¯„ä¼°ã€è¶‹åŠ¿é¢„æµ‹

2. **éœ€è¦è¡¥å……èµ„æ–™çš„æƒ…å†µ**ï¼š
   - åªæœ‰æ³›æ³›çš„è¡Œä¸šä¿¡æ¯ï¼Œç¼ºå°‘å…·ä½“æ¡ˆä¾‹
   - ç¼ºå°‘å…³é”®çš„èƒŒæ™¯ä¿¡æ¯æˆ–æœ€æ–°åŠ¨æ€
   - æ— æ³•åŸºäºç°æœ‰å†…å®¹å†™å‡ºæœ‰ä»·å€¼çš„æ·±åº¦åˆ†æ

**å“åº”æ ¼å¼**ï¼š
- å¦‚æœå¯ä»¥å¼€å§‹å†™æŠ¥å‘Šï¼š<finish />
- å¦‚æœéœ€è¦æœç´¢è¡¥å……ï¼š<keywords_to_search>å…³é”®è¯1,å…³é”®è¯2,å…³é”®è¯3</keywords_to_search>

**æœç´¢å…³é”®è¯è¦æ±‚**ï¼š
- å…·ä½“çš„å…¬å¸åã€äº§å“åã€äº‹ä»¶å
- æœ€æ–°çš„æ–°é—»åŠ¨æ€ã€å‘å¸ƒä¼šã€æ”¿ç­–å˜åŒ–
- é¿å…å®½æ³›è¯æ±‡ï¼Œè¦å…·ä½“åŒ–
- æ¯æ¬¡æœ€å¤š3ä¸ªå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”

è®°ä½ï¼šæŠ¥å‘Šä¸æ˜¯å¤§è€Œå…¨çš„è¡Œä¸šæŠ¥å‘Šï¼Œè€Œæ˜¯åŸºäºæŸä¸ªå…·ä½“ç°è±¡çš„æ·±åº¦åˆ†æã€‚"""
            },
            {
                "role": "user",
                "content": f"""å½“å‰æ˜¯ç¬¬ {iteration} è½®ç ”ç©¶è¿­ä»£ã€‚

{user_prompt_xml}

{kb_xml}

è¯·åˆ†æç°æœ‰çŸ¥è¯†åº“ï¼š
1. å¦‚æœå·²ç»æœ‰è¶³å¤Ÿçš„å…·ä½“æ–°é—»å†…å®¹å¯ä»¥å†™å‡ºè¯¦ç»†åˆ†ææŠ¥å‘Šï¼Œè¯·è¿”å› <finish />
2. å¦‚æœè¿˜ç¼ºå°‘å…³é”®çš„å…·ä½“ä¿¡æ¯ã€æ¡ˆä¾‹ã€æ•°æ®ï¼Œæ‰æœç´¢è¡¥å……

åˆ¤æ–­æ ‡å‡†ï¼š
- æœ‰å…·ä½“çš„æ–°é—»äº‹ä»¶ã€å…¬å¸åŠ¨æ€ã€äº§å“å‘å¸ƒç­‰å†…å®¹ â†’ å¯ä»¥ç»“æŸ
- æœ‰è¶³å¤Ÿçš„ç»†èŠ‚å¯ä»¥è¿›è¡Œæ·±åº¦åˆ†æ â†’ å¯ä»¥ç»“æŸ  
- åªæœ‰æ³›æ³›çš„è¡Œä¸šä¿¡æ¯ï¼Œç¼ºå°‘å…·ä½“æ¡ˆä¾‹ â†’ éœ€è¦æœç´¢"""
            }
        ]
        
        try:
            response = self.llm_service._make_request(messages, temperature=0.3)
            return self._parse_ai_response(response)
        except Exception as e:
            logger.error(f"AIç ”ç©¶æç¤ºå¤±è´¥: {e}")
            return {'action': 'finish', 'details': f'AIè°ƒç”¨å¤±è´¥: {e}'}
    
    def _parse_ai_response(self, response: str) -> Dict[str, Any]:
        """è§£æAIçš„XMLå“åº”"""
        try:
            response = response.strip()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯finish
            if '<finish />' in response or '<finish/>' in response:
                return {'action': 'finish', 'details': 'AIå†³å®šç ”ç©¶å·²å®Œæˆ'}
            
            # è§£ækeywords_to_search
            keywords_match = re.search(r'<keywords_to_search>(.*?)</keywords_to_search>', response, re.DOTALL)
            if keywords_match:
                keywords_text = keywords_match.group(1).strip()
                keywords = [k.strip() for k in keywords_text.split(',') if k.strip()]
                return {
                    'action': 'search',
                    'keywords': keywords,
                    'details': f'éœ€è¦æœç´¢å…³é”®è¯: {keywords}'
                }
            
            # å¦‚æœæ— æ³•è§£æï¼Œé»˜è®¤ç»“æŸ
            logger.warning(f"æ— æ³•è§£æAIå“åº”: {response}")
            return {'action': 'finish', 'details': 'æ— æ³•è§£æAIå“åº”ï¼Œç»“æŸç ”ç©¶'}
            
        except Exception as e:
            logger.error(f"è§£æAIå“åº”å¤±è´¥: {e}")
            return {'action': 'finish', 'details': f'è§£æå¤±è´¥: {e}'}
    
    async def _execute_search_and_crawl(self, keywords: List[str], urls: List[str], settings) -> List[Dict]:
        """æ‰§è¡Œæœç´¢å’Œçˆ¬å–"""
        try:
            # è®¾ç½®æ•´ä¸ªæœç´¢å’Œçˆ¬å–è¿‡ç¨‹çš„è¶…æ—¶æ—¶é—´ä¸º60ç§’
            return await asyncio.wait_for(
                self._execute_search_and_crawl_impl(keywords, urls, settings),
                timeout=60.0
            )
        except asyncio.TimeoutError:
            logger.error("æœç´¢å’Œçˆ¬å–è¿‡ç¨‹è¶…æ—¶ï¼ˆ60ç§’ï¼‰ï¼Œè¿”å›å·²è·å–çš„ç»“æœ")
            return []
        except Exception as e:
            logger.error(f"æœç´¢å’Œçˆ¬å–è¿‡ç¨‹å¤±è´¥: {e}")
            return []
    
    async def _execute_search_and_crawl_impl(self, keywords: List[str], urls: List[str], settings) -> List[Dict]:
        """æ‰§è¡Œæœç´¢å’Œçˆ¬å–çš„å…·ä½“å®ç°"""
        new_articles = []
        
        # 1. æ‰§è¡Œæœç´¢
        for keyword in keywords[:3]:  # é™åˆ¶æœç´¢å…³é”®è¯æ•°é‡
            logger.info(f"æœç´¢å…³é”®è¯: {keyword}")
            
            search_results = self.llm_service.search_web_for_topic(keyword, settings.serp_api_key)
            
            if search_results:
                # è®©AIç­›é€‰è¦çˆ¬å–çš„URL
                urls_to_crawl = await self._ai_select_urls(search_results, keyword)
                
                # çˆ¬å–é€‰ä¸­çš„URLå¹¶å…¥åº“
                for i, url in enumerate(urls_to_crawl[:3]):  # æ¯ä¸ªå…³é”®è¯æœ€å¤šçˆ¬3ä¸ªURL
                    try:
                        # å…ˆæ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“
                        if await self._url_exists_in_db(url):
                            logger.info(f"URLå·²å­˜åœ¨äºæ•°æ®åº“ï¼Œè·³è¿‡: {url}")
                            continue
                            
                        logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {i+1}/{min(len(urls_to_crawl), 3)} ä¸ªURL: {url[:50]}...")
                        result = await self.crawler_service.crawl_article_content(url)
                        if result['success']:
                            # æ·»åŠ åˆ°æ–°æ–‡ç« åˆ—è¡¨
                            article_data = {
                                'title': result['title'],
                                'content': result['content'],
                                'url': result['url'],
                                'author': result.get('author', ''),
                                'date': result.get('date', ''),
                                'source': f'æœç´¢: {keyword}',
                                'crawled_at': datetime.now().isoformat()
                            }
                            new_articles.append(article_data)
                            
                            # ä¿å­˜åˆ°æ•°æ®åº“ - åˆ›å»ºä¸€ä¸ªç‰¹æ®Šçš„"æ·±åº¦ç ”ç©¶"çˆ¬è™«è®°å½•
                            await self._save_search_result_to_db(result, keyword)
                            
                            logger.info(f"æˆåŠŸçˆ¬å–å¹¶å…¥åº“: {result['title'][:50]}...")
                    except Exception as e:
                        logger.error(f"çˆ¬å–URLå¤±è´¥ {url}: {e}")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™åˆ¶
            await asyncio.sleep(2)
        
        return new_articles
    
    async def _url_exists_in_db(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“ä¸­"""
        try:
            from models import CrawlRecord
            existing_record = CrawlRecord.query.filter_by(url=url, status='success').first()
            return existing_record is not None
        except Exception as e:
            logger.error(f"æ£€æŸ¥URLæ˜¯å¦å­˜åœ¨å¤±è´¥: {e}")
            return False
    
    async def _save_search_result_to_db(self, crawl_result: Dict, keyword: str):
        """å°†æœç´¢ç»“æœä¿å­˜åˆ°æ•°æ®åº“"""
        try:
            from models import CrawlerConfig, CrawlRecord, db
            from datetime import datetime
            
            # æŸ¥æ‰¾æˆ–åˆ›å»º"æ·±åº¦ç ”ç©¶"çˆ¬è™«é…ç½®
            deep_research_crawler = CrawlerConfig.query.filter_by(name="æ·±åº¦ç ”ç©¶").first()
            if not deep_research_crawler:
                deep_research_crawler = CrawlerConfig(
                    name="æ·±åº¦ç ”ç©¶",
                    list_url="https://news.google.com/search?q=æ·±åº¦ç ”ç©¶&hl=zh-CN&gl=CN&ceid=CN:zh-Hans",
                    url_regex=r'https://[^/]+/.*',
                    frequency_seconds=0,  # ä¸å®šæ—¶æ‰§è¡Œ
                    is_active=False  # ä¸å‚ä¸å¸¸è§„çˆ¬å–
                )
                db.session.add(deep_research_crawler)
                db.session.flush()  # è·å–ID
            
            # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤
            existing_record = CrawlRecord.query.filter_by(
                url=crawl_result['url'],
                status='success'
            ).first()
            
            if not existing_record:
                # åˆ›å»ºæ–°çš„çˆ¬å–è®°å½•
                record = CrawlRecord(
                    crawler_config_id=deep_research_crawler.id,
                    url=crawl_result['url'],
                    title=crawl_result['title'],
                    content=crawl_result['content'],
                    author=crawl_result.get('author', ''),
                    publish_date=None,  # æœç´¢ç»“æœé€šå¸¸æ²¡æœ‰å‡†ç¡®çš„å‘å¸ƒæ—¥æœŸ
                    status='success'
                )
                db.session.add(record)
                db.session.commit()
                logger.info(f"æœç´¢ç»“æœå·²å…¥åº“: {crawl_result['title'][:30]}...")
            else:
                logger.info(f"URLå·²å­˜åœ¨äºæ•°æ®åº“ï¼Œè·³è¿‡: {crawl_result['url']}")
                
        except Exception as e:
            logger.error(f"ä¿å­˜æœç´¢ç»“æœåˆ°æ•°æ®åº“å¤±è´¥: {e}")
            # ä¸å½±å“ä¸»æµç¨‹ï¼Œç»§ç»­æ‰§è¡Œ
    
    async def _ai_select_urls(self, search_results: List[Dict], keyword: str) -> List[str]:
        """è®©AIä»æœç´¢ç»“æœä¸­é€‰æ‹©è¦çˆ¬å–çš„URLï¼Œä¼˜å…ˆé€‰æ‹©æœ€æ–°çš„å†…å®¹"""
        
        # æŒ‰æ—¥æœŸæ’åºæœç´¢ç»“æœï¼Œä¼˜å…ˆæ˜¾ç¤ºæœ€æ–°çš„
        sorted_results = []
        for result in search_results:
            date_score = 0
            
            # æ£€æŸ¥æ ‡é¢˜å’Œæ‘˜è¦ä¸­çš„æ—¶é—´ç›¸å…³è¯æ±‡
            title = result.get('title', '').lower()
            snippet = result.get('snippet', '').lower()
            
            # æœ€æ–°æ—¶é—´è¯æ±‡å¾—åˆ†æœ€é«˜
            latest_keywords = ['2025', 'æœ€æ–°', 'ä»Šæ—¥', 'åˆšåˆš', 'latest', 'today']
            recent_keywords = ['2024', 'è¿‘æœŸ', 'æœ€è¿‘', 'recent', 'new']
            
            for keyword in latest_keywords:
                if keyword in title or keyword in snippet:
                    date_score += 100
            
            for keyword in recent_keywords:
                if keyword in title or keyword in snippet:
                    date_score += 50
            
            # å¦‚æœæœ‰å…·ä½“æ—¥æœŸï¼Œæ ¹æ®å¹´ä»½ç»™åˆ†
            if result.get('date'):
                date_str = result['date']
                if '2025' in date_str:
                    date_score += 90
                elif '2024' in date_str:
                    date_score += 60
                elif '2023' in date_str:
                    date_score += 30
                else:
                    date_score += 10  # å…¶ä»–å¹´ä»½è¾ƒä½åˆ†æ•°
            
            sorted_results.append((result, date_score))
        
        # æŒ‰åˆ†æ•°æ’åºï¼Œåˆ†æ•°é«˜çš„åœ¨å‰
        sorted_results.sort(key=lambda x: x[1], reverse=True)
        
        # æ„å»ºæœç´¢ç»“æœæè¿°ï¼Œä¼˜å…ˆæ˜¾ç¤ºæœ€æ–°çš„
        results_text = ""
        for i, (result, score) in enumerate(sorted_results[:10], 1):
            results_text += f"{i}. æ ‡é¢˜: {result.get('title', '')}\n"
            results_text += f"   æ¥æº: {result.get('source', '')}\n"
            results_text += f"   æ‘˜è¦: {result.get('snippet', '')}\n"
            results_text += f"   æ—¥æœŸ: {result.get('date', 'æœªçŸ¥')}\n"
            results_text += f"   URL: {result.get('url', '')}\n\n"
        
        messages = [
            {
                "role": "system",
                "content": """ä½ éœ€è¦ä»æœç´¢ç»“æœä¸­é€‰æ‹©æœ€æœ‰ä»·å€¼çš„URLè¿›è¡Œæ·±å…¥çˆ¬å–ã€‚

è¯·è¿”å›XMLæ ¼å¼ï¼š
<urls_to_crawl>
URL1
URL2
URL3
</urls_to_crawl>

é€‰æ‹©æ ‡å‡†ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰ï¼š
1. **æ—¶æ•ˆæ€§ä¼˜å…ˆ** - ä¼˜å…ˆé€‰æ‹©æœ€æ–°çš„å†…å®¹ï¼ˆ2025å¹´ã€2024å¹´ã€åŒ…å«"æœ€æ–°"ç­‰æ—¶é—´è¯æ±‡ï¼‰
2. **æƒå¨æ€§** - å®˜æ–¹ç½‘ç«™ã€çŸ¥ååª’ä½“çš„å†…å®¹
3. **ç›¸å…³æ€§** - ä¸æœç´¢å…³é”®è¯é«˜åº¦ç›¸å…³
4. **æ·±åº¦æ€§** - å¯èƒ½åŒ…å«æ·±å…¥åˆ†æçš„æ–‡ç« 
5. **åªé€‰æ‹©3ä¸ªæœ€æ–°ä¸”æœ€æœ‰ä»·å€¼çš„URL**"""
            },
            {
                "role": "user",
                "content": f"""æœç´¢å…³é”®è¯: {keyword}

æœç´¢ç»“æœï¼ˆå·²æŒ‰æ—¶æ•ˆæ€§æ’åºï¼Œè¶Šé å‰è¶Šæ–°ï¼‰:
{results_text}

è¯·ä¼˜å…ˆé€‰æ‹©æœ€æ–°çš„3ä¸ªURLè¿›è¡Œçˆ¬å–ï¼Œç¡®ä¿è·å–æœ€æ–°ä¿¡æ¯ã€‚"""
            }
        ]
        
        try:
            response = self.llm_service._make_request(messages, temperature=0.2)
            
            # è§£æURLs
            urls_match = re.search(r'<urls_to_crawl>(.*?)</urls_to_crawl>', response, re.DOTALL)
            if urls_match:
                urls_text = urls_match.group(1).strip()
                urls = [url.strip() for url in urls_text.split('\n') if url.strip() and url.strip().startswith('http')]
                return urls[:3]  # æœ€å¤š3ä¸ª
            
            # å¦‚æœAIæ²¡æœ‰è¿”å›ï¼Œé€‰æ‹©æŒ‰æ—¶æ•ˆæ€§æ’åºåçš„å‰3ä¸ª
            return [result[0]['url'] for result in sorted_results[:3] if result[0].get('url')]
            
        except Exception as e:
            logger.error(f"AIé€‰æ‹©URLå¤±è´¥: {e}")
            # é»˜è®¤é€‰æ‹©æŒ‰æ—¶æ•ˆæ€§æ’åºåçš„å‰3ä¸ª
            sorted_results = []
            for result in search_results:
                date_score = 0
                title = result.get('title', '').lower()
                snippet = result.get('snippet', '').lower()
                
                # æœ€æ–°æ—¶é—´è¯æ±‡å¾—åˆ†æœ€é«˜
                latest_keywords = ['2025', 'æœ€æ–°', 'ä»Šæ—¥', 'åˆšåˆš', 'latest', 'today']
                recent_keywords = ['2024', 'è¿‘æœŸ', 'æœ€è¿‘', 'recent', 'new']
                
                for keyword in latest_keywords:
                    if keyword in title or keyword in snippet:
                        date_score += 100
                
                for keyword in recent_keywords:
                    if keyword in title or keyword in snippet:
                        date_score += 50
                
                # å¦‚æœæœ‰å…·ä½“æ—¥æœŸï¼Œæ ¹æ®å¹´ä»½ç»™åˆ†
                if result.get('date'):
                    date_str = result['date']
                    if '2025' in date_str:
                        date_score += 90
                    elif '2024' in date_str:
                        date_score += 60
                    elif '2023' in date_str:
                        date_score += 30
                    else:
                        date_score += 10
                
                sorted_results.append((result, date_score))
            
            sorted_results.sort(key=lambda x: x[1], reverse=True)
            return [result[0]['url'] for result in sorted_results[:3] if result[0].get('url')]
    
    async def _ai_initial_analysis(self, knowledge_base: List[Dict], report_config) -> Dict[str, Any]:
        """AIå¯¹åˆå§‹çŸ¥è¯†åº“è¿›è¡Œåˆ†æï¼Œåˆ¶å®šç ”ç©¶æ–¹å‘"""
        logger.info(f"AIå¼€å§‹åˆ†æ {len(knowledge_base)} ç¯‡åˆå§‹æ–‡ç« ")
        
        try:
            # æ„å»ºæ–‡ç« æ‘˜è¦
            articles_summary = ""
            for i, article in enumerate(knowledge_base[:20], 1):  # åªåˆ†æå‰20ç¯‡
                articles_summary += f"{i}. æ ‡é¢˜: {article.get('title', '')}\n"
                articles_summary += f"   æ¥æº: {article.get('source', '')}\n"
                articles_summary += f"   æ—¥æœŸ: {article.get('date', '')}\n"
                articles_summary += f"   æ‘˜è¦: {article.get('content', '')[:200]}...\n\n"
            
            messages = [
                {
                    "role": "system",
                    "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åˆ†æå¸ˆã€‚è¯·åˆ†æç»™å®šçš„æ–‡ç« é›†åˆï¼Œå¹¶åˆ¶å®šæ·±åº¦ç ”ç©¶è®¡åˆ’ã€‚

è¯·è¿”å›XMLæ ¼å¼ï¼š
<analysis>
<current_knowledge>
ç®€è¦æ€»ç»“ç°æœ‰æ–‡ç« è¦†ç›–çš„ä¸»è¦å†…å®¹å’Œè§‚ç‚¹
</current_knowledge>
<knowledge_gaps>
è¯†åˆ«å‡ºæ˜æ˜¾çš„çŸ¥è¯†ç©ºç™½æˆ–éœ€è¦æ·±å…¥ç ”ç©¶çš„é¢†åŸŸ
</knowledge_gaps>
<research_directions>
å»ºè®®1ï¼šå…·ä½“çš„ç ”ç©¶æ–¹å‘
å»ºè®®2ï¼šå…·ä½“çš„ç ”ç©¶æ–¹å‘
å»ºè®®3ï¼šå…·ä½“çš„ç ”ç©¶æ–¹å‘
</research_directions>
<priority_keywords>
å…³é”®è¯1,å…³é”®è¯2,å…³é”®è¯3
</priority_keywords>
</analysis>

åˆ†ææ ‡å‡†ï¼š
1. è¯†åˆ«ç°æœ‰çŸ¥è¯†çš„å®Œæ•´æ€§
2. å‘ç°éœ€è¦è¡¥å……çš„ä¿¡æ¯ç¼ºå£
3. æå‡ºæœ‰é’ˆå¯¹æ€§çš„ç ”ç©¶æ–¹å‘
4. å»ºè®®ä¼˜å…ˆæœç´¢çš„å…³é”®è¯"""
                },
                {
                    "role": "user",
                    "content": f"""ç ”ç©¶ä¸»é¢˜: {report_config.purpose}
ç ”ç©¶ä¾§é‡ç‚¹: {report_config.research_focus}
ç”¨æˆ·å…³é”®è¯: {report_config.filter_keywords}

ç°æœ‰æ–‡ç« é›†åˆ:
{articles_summary}

è¯·åˆ†æè¿™äº›æ–‡ç« çš„å†…å®¹ï¼Œè¯†åˆ«çŸ¥è¯†ç©ºç™½ï¼Œå¹¶åˆ¶å®šæ·±åº¦ç ”ç©¶è®¡åˆ’ã€‚"""
                }
            ]
            
            logger.info("å¼€å§‹AIåˆæ­¥åˆ†æï¼Œä½¿ç”¨æµå¼è¿”å›...")
            response = self.llm_service._make_request(messages, temperature=0.3, stream=True)
            if not response:
                return {'summary': 'åˆæ­¥åˆ†æå¤±è´¥', 'directions': [], 'keywords': []}
            
            # è§£æAIçš„åˆ†æç»“æœ
            analysis_result = self._parse_initial_analysis(response)
            
            logger.info(f"AIåˆ†æç»“æœ: {len(analysis_result.get('directions', []))} ä¸ªç ”ç©¶æ–¹å‘")
            return analysis_result
            
        except Exception as e:
            logger.error(f"AIåˆæ­¥åˆ†æå¤±è´¥: {e}")
            return {'summary': f'åˆ†æå¤±è´¥: {e}', 'directions': [], 'keywords': []}
    
    def _parse_initial_analysis(self, ai_response: str) -> Dict[str, Any]:
        """è§£æAIçš„åˆæ­¥åˆ†æç»“æœ"""
        try:
            import re
            
            result = {
                'summary': '',
                'gaps': '',
                'directions': [],
                'keywords': []
            }
            
            # æå–current_knowledge
            knowledge_match = re.search(r'<current_knowledge>(.*?)</current_knowledge>', ai_response, re.DOTALL)
            if knowledge_match:
                result['summary'] = knowledge_match.group(1).strip()
            
            # æå–knowledge_gaps
            gaps_match = re.search(r'<knowledge_gaps>(.*?)</knowledge_gaps>', ai_response, re.DOTALL)
            if gaps_match:
                result['gaps'] = gaps_match.group(1).strip()
            
            # æå–research_directions
            directions_match = re.search(r'<research_directions>(.*?)</research_directions>', ai_response, re.DOTALL)
            if directions_match:
                directions_text = directions_match.group(1).strip()
                # æŒ‰è¡Œåˆ†å‰²ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªç ”ç©¶æ–¹å‘
                directions = [d.strip() for d in directions_text.split('\n') if d.strip()]
                result['directions'] = directions
            
            # æå–priority_keywords
            keywords_match = re.search(r'<priority_keywords>(.*?)</priority_keywords>', ai_response, re.DOTALL)
            if keywords_match:
                keywords_text = keywords_match.group(1).strip()
                keywords = [k.strip() for k in keywords_text.split(',') if k.strip()]
                result['keywords'] = keywords
            
            return result
            
        except Exception as e:
            logger.error(f"è§£æAIåˆ†æç»“æœå¤±è´¥: {e}")
            return {'summary': f'è§£æå¤±è´¥: {e}', 'directions': [], 'keywords': []}
    
    async def _send_guided_research_prompt(self, knowledge_base: List[Dict], report_config, initial_analysis: Dict, iteration: int) -> Dict[str, Any]:
        """å‘é€åŸºäºåˆæ­¥åˆ†æçš„ç ”ç©¶æç¤ºç»™AI"""
        logger.info(f"ç¬¬ {iteration} è½®ï¼šå‘é€æŒ‡å¯¼ç ”ç©¶æç¤ºç»™AI")
        
        try:
            # æ„å»ºå½“å‰çŸ¥è¯†åº“æ¦‚è¦ï¼ˆä¸å‘é€å…¨éƒ¨å†…å®¹ï¼Œåªå‘é€æ‘˜è¦ï¼‰
            kb_summary = f"å½“å‰çŸ¥è¯†åº“åŒ…å« {len(knowledge_base)} ç¯‡æ–‡ç« ï¼Œæœ€æ–°çš„5ç¯‡ï¼š\n"
            for i, article in enumerate(knowledge_base[-5:], 1):
                kb_summary += f"{i}. {article.get('title', '')[:50]}... (æ¥æº: {article.get('source', '')})\n"
            
            messages = [
                {
                    "role": "system", 
                    "content": """ä½ æ˜¯é€šç”¨ç ”ç©¶AIï¼Œæ“…é•¿æŒ‡å¯¼å„é¢†åŸŸæ·±åº¦ç ”ç©¶ã€‚åŸºäºå½“å‰ç ”ç©¶çŠ¶æ€ï¼Œæ™ºèƒ½å†³å®šä¸‹ä¸€æ­¥æ–¹å‘ã€‚

ğŸ¯ å†³ç­–æ¡†æ¶ï¼ˆé€‚ç”¨ä»»ä½•ç ”ç©¶é¢†åŸŸï¼‰ï¼š
- ç°çŠ¶è¯„ä¼°ï¼šå½“å‰ä¿¡æ¯å®Œæ•´åº¦ã€è´¨é‡ã€è¦†ç›–é¢
- ç¼ºå£è¯†åˆ«ï¼šå…³é”®ä¿¡æ¯ç¼ºå¤±ã€æ·±åº¦ä¸è¶³ã€è§’åº¦å•ä¸€
- ä¼˜å…ˆçº§æ’åºï¼šé‡è¦æ€§ã€ç´§è¿«æ€§ã€å¯è·å¾—æ€§
- æ•ˆæœé¢„æµ‹ï¼šæœç´¢é¢„æœŸæ”¶ç›Šã€æˆæœ¬æŠ•å…¥æ¯”

ğŸ“‹ è¿”å›æ ¼å¼ï¼š
ç»§ç»­ç ”ç©¶æ—¶ï¼š
<research_decision>
<action>search</action>
<keywords>æ ¸å¿ƒå…³é”®è¯,ç›¸å…³æœ¯è¯­,å»¶ä¼¸æ¦‚å¿µ</keywords>
<reasoning>åŸºäºç¼ºå£åˆ†æçš„æœç´¢ç†ç”±</reasoning>
</research_decision>

å®Œæˆç ”ç©¶æ—¶ï¼š
<research_decision>
<action>finish</action>
<reasoning>ä¿¡æ¯å……è¶³å¯ç”Ÿæˆé«˜è´¨é‡æŠ¥å‘Šçš„åˆ¤æ–­ä¾æ®</reasoning>
</research_decision>

ğŸ” é€šç”¨å†³ç­–åŸåˆ™ï¼š
1. ä¼˜å…ˆå¡«è¡¥å½±å“ç»“è®ºçš„å…³é”®ä¿¡æ¯ç©ºç™½
2. é¿å…ä¿¡æ¯é‡å¤ï¼Œè¿½æ±‚å†…å®¹å¢é‡ä»·å€¼
3. å¹³è¡¡æ·±åº¦ä¸å¹¿åº¦ï¼Œç¡®ä¿åˆ†æç»´åº¦å®Œæ•´
4. å½“ä¿¡æ¯è¶³ä»¥æ”¯æ’‘ä¸“ä¸šæŠ¥å‘Šæ—¶åŠæ—¶ç»“æŸ"""
                },
                {
                    "role": "user",
                    "content": f"""ç ”ç©¶ä¸»é¢˜: {report_config.purpose}
ç ”ç©¶ä¾§é‡ç‚¹: {report_config.research_focus}

=== åˆæ­¥åˆ†æç»“æœ ===
ç°æœ‰çŸ¥è¯†: {initial_analysis.get('summary', '')}
çŸ¥è¯†ç©ºç™½: {initial_analysis.get('gaps', '')}
å»ºè®®æ–¹å‘: {'; '.join(initial_analysis.get('directions', []))}
ä¼˜å…ˆå…³é”®è¯: {', '.join(initial_analysis.get('keywords', []))}

=== å½“å‰ç ”ç©¶çŠ¶æ€ ===
{kb_summary}
å½“å‰æ˜¯ç¬¬ {iteration} è½®ç ”ç©¶

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚"""
                }
            ]
            
            response = self.llm_service._make_request(messages, temperature=0.3, stream=False)  # å†³ç­–ä¸éœ€è¦æµå¼
            if not response:
                return {'action': 'finish', 'details': 'AIæ— å“åº”ï¼Œç»“æŸç ”ç©¶'}
            
            # è§£æAIå†³ç­–
            decision = self._parse_research_decision(response)
            
            logger.info(f"ç¬¬ {iteration} è½®AIå†³ç­–: {decision.get('action', 'unknown')}")
            return decision
            
        except Exception as e:
            logger.error(f"å‘é€æŒ‡å¯¼ç ”ç©¶æç¤ºå¤±è´¥: {e}")
            return {'action': 'finish', 'details': f'å‘é€æç¤ºå¤±è´¥: {e}'}
    
    def _parse_research_decision(self, ai_response: str) -> Dict[str, Any]:
        """è§£æAIçš„ç ”ç©¶å†³ç­–"""
        try:
            import re
            
            result = {'action': 'finish', 'details': 'AIå†³ç­–è§£æå¤±è´¥'}
            
            # æå–action
            action_match = re.search(r'<action>(.*?)</action>', ai_response, re.DOTALL)
            if action_match:
                result['action'] = action_match.group(1).strip().lower()
            
            # æå–reasoning
            reasoning_match = re.search(r'<reasoning>(.*?)</reasoning>', ai_response, re.DOTALL)
            if reasoning_match:
                result['details'] = reasoning_match.group(1).strip()
            
            # å¦‚æœæ˜¯æœç´¢åŠ¨ä½œï¼Œæå–å…³é”®è¯
            if result['action'] == 'search':
                keywords_match = re.search(r'<keywords>(.*?)</keywords>', ai_response, re.DOTALL)
                if keywords_match:
                    keywords_text = keywords_match.group(1).strip()
                    keywords = [k.strip() for k in keywords_text.split(',') if k.strip()]
                    result['keywords'] = keywords
                else:
                    result['keywords'] = []
            
            return result
            
        except Exception as e:
            logger.error(f"è§£æAIç ”ç©¶å†³ç­–å¤±è´¥: {e}")
            return {'action': 'finish', 'details': f'è§£æå¤±è´¥: {e}'}
    
    async def _generate_final_report(self, knowledge_base: List[Dict], report_config) -> str:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        logger.info(f"åŸºäº {len(knowledge_base)} ç¯‡æ–‡ç« ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
        
        # æ„å»ºå®Œæ•´çš„knowledge_base XML
        kb_xml = self._build_knowledge_base_xml(knowledge_base)
        
        messages = [
            {
                "role": "system",
                "content": f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆï¼Œæ“…é•¿å¯¹å…·ä½“æ–°é—»äº‹ä»¶è¿›è¡Œæ·±åº¦åˆ†æã€‚è¯·åŸºäºçŸ¥è¯†åº“ä¸­çš„å…·ä½“æ–°é—»å†…å®¹ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Šã€‚

ğŸ¯ **æŠ¥å‘Šè¦æ±‚**ï¼š
- **èšç„¦å…·ä½“äº‹ä»¶**ï¼šé’ˆå¯¹çŸ¥è¯†åº“ä¸­çš„å…·ä½“æ–°é—»ã€æ¡ˆä¾‹ã€äº‹ä»¶è¿›è¡Œåˆ†æï¼Œä¸è¦å†™æ³›æ³›è€Œè°ˆçš„è¡Œä¸šæŠ¥å‘Š
- **è¯¦ç»†å†…å®¹åˆ†æ**ï¼šæ·±å…¥åˆ†ææ–°é—»èƒŒæ™¯ã€å…³é”®ä¿¡æ¯ã€å½±å“å› ç´ ï¼Œæä¾›å…·ä½“çš„æ•°æ®å’Œç»†èŠ‚
- **å¤šè§’åº¦è§£è¯»**ï¼šä»æŠ€æœ¯ã€å•†ä¸šã€å¸‚åœºã€ç”¨æˆ·ç­‰å¤šä¸ªè§’åº¦åˆ†æäº‹ä»¶
- **å®ç”¨æ€§å¼º**ï¼šæä¾›å¯æ“ä½œçš„æ´å¯Ÿå’Œå»ºè®®
- **å¼•ç”¨æ¥æº**ï¼šåœ¨å¼•ç”¨å…·ä½“ä¿¡æ¯æ—¶ï¼Œå¿…é¡»æ·»åŠ æ¥æºé“¾æ¥ï¼Œæ ¼å¼ä¸º [æ¥æº](URL)

ğŸ“‹ **åˆ†ææ¡†æ¶**ï¼š
1. **äº‹ä»¶æ¦‚è¿°** - å…·ä½“å‘ç”Ÿäº†ä»€ä¹ˆï¼Œå…³é”®å‚ä¸è€…ï¼Œæ—¶é—´èƒŒæ™¯
2. **æ·±åº¦åˆ†æ** - äº‹ä»¶èƒŒåçš„åŸå› ã€æŠ€æœ¯ç»†èŠ‚ã€å•†ä¸šé€»è¾‘
3. **å½±å“è¯„ä¼°** - å¯¹è¡Œä¸šã€ç”¨æˆ·ã€ç«äº‰å¯¹æ‰‹çš„å…·ä½“å½±å“
4. **è¶‹åŠ¿é¢„æµ‹** - åŸºäºæ­¤äº‹ä»¶å¯èƒ½çš„åç»­å‘å±•
5. **å®ç”¨å»ºè®®** - é’ˆå¯¹ä¸åŒè§’è‰²çš„å…·ä½“å»ºè®®

âš ï¸ **é¿å…**ï¼š
- ä¸è¦å†™æˆè¡Œä¸šæ¦‚è¿°æˆ–é€šç”¨åˆ†æ
- ä¸è¦åªåˆ—æ ‡é¢˜å’Œå¤§çº²
- ä¸è¦æ³›æ³›è€Œè°ˆï¼Œè¦æœ‰å…·ä½“å†…å®¹
- æ¯ä¸ªéƒ¨åˆ†éƒ½è¦æœ‰è¯¦ç»†çš„åˆ†æå†…å®¹

ğŸ” å½“å‰åˆ†æä»»åŠ¡ï¼š
- åˆ†æç›®çš„ï¼š{report_config.purpose}
- åˆ†æé‡ç‚¹ï¼š{report_config.research_focus}"""
            },
            {
                "role": "user",
                "content": f"""è¯·åŸºäºä»¥ä¸‹çŸ¥è¯†åº“ä¸­çš„å…·ä½“æ–°é—»å†…å®¹ï¼Œç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Šã€‚

è¦æ±‚ï¼š
1. é‡ç‚¹åˆ†æçŸ¥è¯†åº“ä¸­çš„å…·ä½“æ–°é—»äº‹ä»¶ï¼Œä¸è¦å†™æ³›æ³›çš„è¡Œä¸šæŠ¥å‘Š
2. æ¯ä¸ªç« èŠ‚éƒ½è¦æœ‰è¯¦ç»†çš„å†…å®¹ï¼Œä¸è¦åªæ˜¯æ ‡é¢˜
3. æä¾›å…·ä½“çš„æ•°æ®ã€æ¡ˆä¾‹ã€åˆ†æ
4. é’ˆå¯¹å…·ä½“äº‹ä»¶è¿›è¡Œæ·±åº¦è§£è¯»
5. **é‡è¦**ï¼šå¼•ç”¨å…·ä½“ä¿¡æ¯æ—¶ï¼Œå¿…é¡»æ·»åŠ æ¥æºé“¾æ¥ï¼Œæ ¼å¼ä¸º [æ¥æº](URL)
6. æ¯ä¸ªé‡è¦è§‚ç‚¹éƒ½è¦æœ‰å¯¹åº”çš„æ¥æºé“¾æ¥æ”¯æ’‘

<user_prompt>
<report_purpose>{report_config.purpose}</report_purpose>
<research_focus>{report_config.research_focus}</research_focus>
</user_prompt>

{kb_xml}

è¯·ç”Ÿæˆè¯¦ç»†çš„æ–°é—»åˆ†ææŠ¥å‘Šï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½è¦æœ‰å…·ä½“å†…å®¹ã€‚"""
            }
        ]
        
        try:
            logger.info("å¼€å§‹ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼Œä½¿ç”¨æµå¼è¿”å›...")
            result = self.llm_service._make_request(messages, temperature=0.7, stream=True)
            
            # æ·»åŠ æŠ¥å‘Šå¤´éƒ¨ä¿¡æ¯
            header = f"""# {report_config.name} - æ·±åº¦ç ”ç©¶æŠ¥å‘Š **[AIç”Ÿæˆ]**

**[æŠ¥å‘Šä¿¡æ¯]**
- **ç”Ÿæˆæ—¶é—´ï¼š** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} **[ç³»ç»Ÿæ—¶é—´]**
- **ç ”ç©¶ç›®çš„ï¼š** {report_config.purpose} **[ç”¨æˆ·é…ç½®]**
- **ç ”ç©¶é‡ç‚¹ï¼š** {report_config.research_focus} **[ç”¨æˆ·é…ç½®]**
- **çŸ¥è¯†åº“è§„æ¨¡ï¼š** {len(knowledge_base)} ç¯‡æ–‡ç«  **[æ•°æ®è§„æ¨¡]**
- **ç ”ç©¶æ¨¡å¼ï¼š** AIæ·±åº¦ç ”ç©¶ (V3.0 XMLäº¤äº’ç‰ˆ) **[æŠ€æœ¯ç‰ˆæœ¬]**

**[è´¨é‡ä¿è¯]**
- âœ… å¤šæºæ•°æ®éªŒè¯ **[æ•°æ®è´¨é‡]**
- âœ… AIæ™ºèƒ½ç­›é€‰è¿‡æ»¤ **[ç®—æ³•å¤„ç†]**
- âœ… æ—¶æ•ˆæ€§æ£€æŸ¥ï¼ˆ7å¤©å†…ï¼‰ **[æ—¶é—´æ§åˆ¶]**
- âœ… æƒå¨æ€§æ¥æºä¼˜å…ˆ **[æ¥æºæ’åº]**

---

"""
            
            footer = f"""

---

## ğŸ“Š æ•°æ®æ¥æº **[é€æ˜åº¦]**

**[ç»Ÿè®¡ä¿¡æ¯]** æœ¬æŠ¥å‘ŠåŸºäº {len(knowledge_base)} ç¯‡æ–‡ç« åˆ†æï¼š

"""
            
            # æŒ‰æ¥æºåˆ†ç±»ç»Ÿè®¡
            source_stats = {}
            for article in knowledge_base:
                source = article.get('source', 'æœªçŸ¥æ¥æº')
                source_stats[source] = source_stats.get(source, 0) + 1
            
            for source, count in source_stats.items():
                footer += f"- **{source}**: {count} ç¯‡ **[æ•°æ®æ¥æº]**\n"
            
            footer += f"""
### å¼•ç”¨åˆ—è¡¨ **[å®Œæ•´è¿½æº¯]**

"""
            
            # æ·»åŠ æ–‡ç« åˆ—è¡¨
            for i, article in enumerate(knowledge_base[:20], 1):  # æœ€å¤šæ˜¾ç¤º20ç¯‡
                footer += f"{i}. [{article.get('title', 'æ— æ ‡é¢˜')}]({article.get('url', '#')}) - {article.get('source', 'æœªçŸ¥æ¥æº')} **[å¼•ç”¨{i}]**\n"
            
            if len(knowledge_base) > 20:
                footer += f"\n*å¦æœ‰ {len(knowledge_base) - 20} ç¯‡æ–‡ç« * **[çœç•¥æ˜¾ç¤º]**\n"
            
            footer += f"""

---

## ğŸ” ç ”ç©¶æ–¹æ³• **[æµç¨‹è¯´æ˜]**

**[å¤„ç†æµç¨‹]**
1. **æ•°æ®æ”¶é›†** - å¤šæºçˆ¬å– **[è‡ªåŠ¨åŒ–]**
2. **è´¨é‡ç­›é€‰** - AIè¿‡æ»¤ + å…³é”®è¯åŒ¹é… **[ç®—æ³•ç­›é€‰]**
3. **æ·±åº¦åˆ†æ** - å¤šè½®è¿­ä»£ **[AIåˆ†æ]**
4. **æŠ¥å‘Šç”Ÿæˆ** - ç»“æ„åŒ–è¾“å‡º **[æ ¼å¼åŒ–]**

**[è´¨é‡æ ‡å‡†]**
- âœ… æ—¶æ•ˆæ€§æ§åˆ¶ **[æ—¶é—´èŒƒå›´]**
- âœ… æƒå¨æ€§ä¼˜å…ˆ **[æ¥æºæ’åº]**
- âœ… å®Œæ•´æ€§ä¿è¯ **[å¤šè§’åº¦]**
- âœ… å‡†ç¡®æ€§éªŒè¯ **[äº¤å‰éªŒè¯]**

*æœ¬æŠ¥å‘ŠAIç”Ÿæˆï¼Œæ ‡æ³¨ç¡®ä¿è¿‡ç¨‹å¯è¿½æº¯* **[ç³»ç»Ÿè¯´æ˜]**"""
            
            return header + result + footer
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå¤±è´¥: {e}")
            return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼š{e}"
    
    async def _send_report_notification(self, report_content: str, report_config) -> bool:
        """å‘é€æŠ¥å‘Šé€šçŸ¥åˆ°ç¾¤ç»„"""
        try:
            logger.info(f"å¼€å§‹æ¨é€æ·±åº¦ç ”ç©¶æŠ¥å‘Šåˆ° {getattr(report_config, 'notification_type', 'wechat')}")
            
            # æ ¼å¼åŒ–æŠ¥å‘Šå†…å®¹ç”¨äºé€šçŸ¥
            formatted_content = self.notification_service.format_deep_research_for_notification(report_content)
            
            # ç”Ÿæˆé€šçŸ¥æ ‡é¢˜
            title = f"ğŸ”¬ {report_config.name} - æ·±åº¦ç ”ç©¶æŠ¥å‘Š"
            
            # å‘é€é€šçŸ¥
            success = self.notification_service.send_notification(
                notification_type=getattr(report_config, 'notification_type', 'wechat'),
                webhook_url=report_config.webhook_url,
                content=formatted_content,
                title=title
            )
            
            if success:
                logger.info("æ·±åº¦ç ”ç©¶æŠ¥å‘Šæ¨é€æˆåŠŸ")
                # ä¿å­˜æ¨é€çŠ¶æ€åˆ°æ•°æ®åº“
                await self._save_notification_status(report_config, True, report_content)
            else:
                logger.error("æ·±åº¦ç ”ç©¶æŠ¥å‘Šæ¨é€å¤±è´¥")
                await self._save_notification_status(report_config, False, report_content)
            
            return success
            
        except Exception as e:
            logger.error(f"æ¨é€æŠ¥å‘Šé€šçŸ¥å¤±è´¥: {e}")
            await self._save_notification_status(report_config, False, report_content, str(e))
            return False
    
    async def _save_notification_status(self, report_config, success: bool, report_content: str, error_msg: str = None):
        """ä¿å­˜é€šçŸ¥çŠ¶æ€åˆ°æ•°æ®åº“"""
        try:
            from models import ReportRecord, db
            from app import app
            
            # åˆ›å»ºæŠ¥å‘Šè®°å½•
            report_record = ReportRecord(
                report_config_id=getattr(report_config, 'id', 0),
                title=report_config.name,
                content=report_content,
                summary=self._extract_report_summary(report_content),
                status='success' if success else 'failed',
                notification_sent=success,
                error_message=error_msg
            )
            
            with app.app_context():
                db.session.add(report_record)
                db.session.commit()
                
                logger.info(f"æŠ¥å‘Šè®°å½•å·²ä¿å­˜: ID={report_record.id}")
                
        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥å‘Šè®°å½•å¤±è´¥: {e}")
    
    def _extract_report_summary(self, report_content: str) -> str:
        """æå–æŠ¥å‘Šæ‘˜è¦"""
        try:
            # ç®€å•æå–å‰200å­—ç¬¦ä½œä¸ºæ‘˜è¦
            lines = report_content.split('\n')
            summary_lines = []
            
            for line in lines:
                if line.strip() and not line.startswith('#'):
                    summary_lines.append(line.strip())
                    if len('\n'.join(summary_lines)) > 200:
                        break
            
            summary = '\n'.join(summary_lines)
            if len(summary) > 200:
                summary = summary[:200] + "..."
            
            return summary
            
        except Exception:
            return "æ— æ³•æå–æ‘˜è¦"
    
    def _build_knowledge_base_xml(self, knowledge_base: List[Dict]) -> str:
        """æ„å»ºknowledge_base XML"""
        xml_content = "<knowledge_base>\n"
        
        for i, article in enumerate(knowledge_base, 1):
            xml_content += f"<article id='{i}'>\n"
            xml_content += f"<title>{self._escape_xml(article.get('title', ''))}</title>\n"
            xml_content += f"<url>{self._escape_xml(article.get('url', ''))}</url>\n"
            xml_content += f"<source>{self._escape_xml(article.get('source', ''))}</source>\n"
            xml_content += f"<date>{self._escape_xml(article.get('date', ''))}</date>\n"
            xml_content += f"<content>{self._escape_xml(article.get('content', '')[:2000])}</content>\n"  # é™åˆ¶é•¿åº¦
            xml_content += f"</article>\n"
        
        xml_content += "</knowledge_base>"
        return xml_content
    
    def _escape_xml(self, text: str) -> str:
        """è½¬ä¹‰XMLç‰¹æ®Šå­—ç¬¦"""
        if not text:
            return ""
        return (text.replace('&', '&amp;')
                   .replace('<', '&lt;')
                   .replace('>', '&gt;')
                   .replace('"', '&quot;')
                   .replace("'", '&#39;'))
    
    def _parse_time_range(self, time_range: str) -> int:
        """è§£ææ—¶é—´èŒƒå›´ä¸ºå°æ—¶æ•°"""
        if time_range == '24h':
            return 24
        elif time_range == '3d':
            return 72
        elif time_range == '7d':
            return 168
        elif time_range == '30d':
            return 720
        else:
            return 24  # é»˜è®¤24å°æ—¶

from datetime import timedelta
