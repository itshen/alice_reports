#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬è™«æœåŠ¡
åŸºäºCrawl4AIå®ç°ç½‘é¡µå†…å®¹æŠ“å–
"""

import asyncio
import re
import logging
from datetime import datetime, timedelta
from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
import json

# ä½¿ç”¨æ ‡å‡†åº“ï¼Œé¿å…ä¾èµ–é—®é¢˜
try:
    from dateutil import parser as date_parser
    DATEUTIL_AVAILABLE = True
except ImportError:
    DATEUTIL_AVAILABLE = False

logger = logging.getLogger(__name__)

class CrawlerService:
    """çˆ¬è™«æœåŠ¡ç±»"""
    
    def __init__(self):
        self.crawler = None
    
    async def test_connection(self, url):
        """æµ‹è¯•è¿æ¥å¹¶è¿”å›é¡µé¢å†…å®¹"""
        try:
            # è®¾ç½®5ç§’è¶…æ—¶
            async with AsyncWebCrawler(verbose=False) as crawler:
                result = await asyncio.wait_for(
                    crawler.arun(url=url), 
                    timeout=5.0
                )
                
                if result.success:
                    # æå–é¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥
                    links = []
                    if result.links:
                        internal_links = result.links.get('internal', [])
                        external_links = result.links.get('external', [])
                        links = internal_links + external_links
                    
                    return {
                        'success': True,
                        'content': result.markdown,  # è¿”å›markdownå†…å®¹ç”¨äºæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼ˆæ¢å¤åŸé€»è¾‘ï¼‰
                        'links': links[:100],  # é™åˆ¶é“¾æ¥æ•°é‡
                        'title': result.metadata.get('title', ''),
                        'html': result.html[:10000]  # é™åˆ¶HTMLé•¿åº¦
                    }
                else:
                    return {
                        'success': False,
                        'error': result.error_message or 'è¿æ¥å¤±è´¥'
                    }
        
        except asyncio.TimeoutError:
            logger.error(f"è¿æ¥è¶…æ—¶: {url}")
            return {
                'success': False,
                'error': f'è¿æ¥è¶…æ—¶ï¼ˆ5ç§’ï¼‰: {url}'
            }
        except Exception as e:
            logger.error(f"æµ‹è¯•è¿æ¥å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def extract_urls_from_page(self, url, regex_pattern):
        """ä»é¡µé¢æå–URLåˆ—è¡¨"""
        try:
            logger.info(f"å¼€å§‹æå–URL from: {url}")
            async with AsyncWebCrawler(verbose=False) as crawler:
                result = await asyncio.wait_for(
                    crawler.arun(url=url), 
                    timeout=5.0
                )
                
                if not result.success:
                    return []
                
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åœ¨markdownä¸­åŒ¹é…URLï¼ˆæ¢å¤åŸé€»è¾‘ï¼‰
                pattern = re.compile(regex_pattern)
                matches = pattern.findall(result.markdown)
                
                # è°ƒè¯•æ—¥å¿—
                logger.info(f"æ­£åˆ™è¡¨è¾¾å¼: {regex_pattern}")
                logger.info(f"Markdownå†…å®¹é•¿åº¦: {len(result.markdown)}")
                logger.info(f"åŒ¹é…åˆ°çš„URLæ•°é‡: {len(matches)}")
                if matches:
                    logger.info(f"å‰3ä¸ªåŒ¹é…çš„URL: {matches[:3]}")
                
                # å»é‡å¹¶è¿”å›
                unique_matches = list(set(matches))
                logger.info(f"å»é‡åURLæ•°é‡: {len(unique_matches)}")
                return unique_matches
        
        except asyncio.TimeoutError:
            logger.error(f"æå–URLè¶…æ—¶: {url}")
            return []
        except Exception as e:
            logger.error(f"æå–URLå¤±è´¥: {e}")
            return []
    
    async def crawl_article_content(self, url):
        """æŠ“å–æ–‡ç« è¯¦ç»†å†…å®¹"""
        try:
            logger.info(f"å¼€å§‹çˆ¬å–æ–‡ç« å†…å®¹: {url}")
            
            # ç»Ÿä¸€è®¾ç½®5ç§’è¶…æ—¶
            timeout = 5.0
            logger.info(f"è®¾ç½®è¶…æ—¶æ—¶é—´: {timeout}ç§’")
            
            # å®šä¹‰æ›´ç²¾ç¡®çš„æ–‡ç« å†…å®¹æå–ç­–ç•¥
            schema = {
                "name": "æ–‡ç« å†…å®¹",
                "baseSelector": "article, .article, .content, .post, main, .article-content, .news-content, .post-body",
                "fields": [
                    {
                        "name": "title",
                        "selector": "h1, h2, .title, .headline, .article-title, .news-title, .post-title",
                        "type": "text"
                    },
                    {
                        "name": "content",
                        "selector": ".content, .article-body, .post-content, .news-body, .article-text, .main-content, .entry-content",
                        "type": "text"
                    },
                    {
                        "name": "author",
                        "selector": ".author, .byline, .writer, .article-author, .news-author",
                        "type": "text"
                    },
                    {
                        "name": "date",
                        "selector": ".date, .publish-date, .article-date, time, .news-date, .post-date, .timestamp, .time, .published, .created, .article-time, .post-time, [datetime], .meta-time, .publish-time",
                        "type": "text"
                    }
                ]
            }
            
            extraction_strategy = JsonCssExtractionStrategy(schema, verbose=False)
            
            async with AsyncWebCrawler(verbose=False) as crawler:
                result = await asyncio.wait_for(
                    crawler.arun(
                        url=url,
                        extraction_strategy=extraction_strategy,
                        # ç§»é™¤æ›´å¤šä¸éœ€è¦çš„å…ƒç´ 
                        excluded_tags=[
                            'nav', 'footer', 'aside', 'script', 'style', 'header', 
                            'menu', 'sidebar', 'advertisement', 'ad', 'banner',
                            'breadcrumb', 'pagination', 'related', 'comment',
                            'social', 'share', 'widget', 'toolbar'
                        ],
                        # ç­‰å¾…é¡µé¢åŠ è½½
                        wait_for="body"
                    ),
                    timeout=timeout  # æ ¹æ®åŸŸååŠ¨æ€è®¾ç½®è¶…æ—¶æ—¶é—´
                )
                
                if result.success:
                    # è§£ææå–çš„ç»“æ„åŒ–æ•°æ®
                    extracted_data = {}
                    if result.extracted_content:
                        try:
                            data_list = json.loads(result.extracted_content)
                            if data_list and isinstance(data_list, list) and len(data_list) > 0:
                                extracted_data = data_list[0]  # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„ç»“æœ
                        except (json.JSONDecodeError, TypeError, IndexError) as e:
                            logger.debug(f"è§£ææå–å†…å®¹å¤±è´¥: {e}")
                            pass
                    
                    # æ¸…ç†å’Œä¼˜åŒ–å†…å®¹
                    content = extracted_data.get('content', '')
                    if not content:
                        # å¦‚æœç»“æ„åŒ–æå–å¤±è´¥ï¼Œå°è¯•ä»markdownä¸­æå–çº¯æ–‡æœ¬å†…å®¹
                        content = self._extract_clean_content_from_markdown(result.markdown)
                    else:
                        # æ¸…ç†å·²æå–çš„å†…å®¹
                        content = self._clean_article_content(content)
                    
                    title = extracted_data.get('title', '') or (result.metadata.get('title', '') if result.metadata else '')
                    
                    logger.info(f"æˆåŠŸçˆ¬å–æ–‡ç« : {title[:50]}...")
                    return {
                        'success': True,
                        'url': url,
                        'title': title,
                        'content': content,
                        'author': extracted_data.get('author', ''),
                        'date': extracted_data.get('date', ''),
                        'markdown': result.markdown
                    }
                else:
                    return {
                        'success': False,
                        'url': url,
                        'error': result.error_message or 'æŠ“å–å¤±è´¥'
                    }
        
        except asyncio.TimeoutError:
            logger.error(f"æŠ“å–æ–‡ç« å†…å®¹è¶…æ—¶ {url}")
            return {
                'success': False,
                'url': url,
                'error': f'æŠ“å–è¶…æ—¶ï¼ˆ5ç§’ï¼‰: {url}'
            }
        except Exception as e:
            logger.error(f"æŠ“å–æ–‡ç« å†…å®¹å¤±è´¥ {url}: {e}")
            return {
                'success': False,
                'url': url,
                'error': str(e)
            }
    
    def _clean_article_content(self, content):
        """æ¸…ç†æ–‡ç« å†…å®¹ï¼Œç§»é™¤ä¸ç›¸å…³ä¿¡æ¯"""
        if not content:
            return ""
        
        # ç§»é™¤å¸¸è§çš„å¯¼èˆªå’Œæ— å…³å†…å®¹
        unwanted_patterns = [
            r'é¦–é¡µ.*?ç½‘ç«™åœ°å›¾',  # å¯¼èˆªèœå•
            r'ç½‘ç«™åœ°å›¾.*?åœ°æ–¹é¢‘é“',  # ç½‘ç«™å¯¼èˆª
            r'åœ°æ–¹é¢‘é“.*?å¤šè¯­ç§é¢‘é“',  # åœ°æ–¹é¢‘é“åˆ—è¡¨
            r'å¤šè¯­ç§é¢‘é“.*?æ–°åæŠ¥åˆŠ',  # å¤šè¯­ç§å¯¼èˆª
            r'æ–°åæŠ¥åˆŠ.*?æ‰¿å»ºç½‘ç«™',  # æŠ¥åˆŠåˆ—è¡¨
            r'æ‰¿å»ºç½‘ç«™.*?å®¢æˆ·ç«¯',  # æ‰¿å»ºç½‘ç«™åˆ—è¡¨
            r'æ‰‹æœºç‰ˆ.*?ç«™å†…æœç´¢',  # ç§»åŠ¨ç‰ˆå¯¼èˆª
            r'Copyright.*?All Rights Reserved',  # ç‰ˆæƒä¿¡æ¯
            r'åˆ¶ä½œå•ä½ï¼š.*?ç‰ˆæƒæ‰€æœ‰ï¼š.*?',  # ç‰ˆæƒä¿¡æ¯
            r'\[.*?\]',  # æ–¹æ‹¬å·å†…å®¹ï¼ˆé€šå¸¸æ˜¯é“¾æ¥æ–‡æœ¬ï¼‰
            r'javascript:void\([^)]*\)',  # JavaScripté“¾æ¥
            r'https?://[^\s]+',  # æ¸…ç†æ®‹ç•™çš„URL
            r'_[^_]*_',  # ä¸‹åˆ’çº¿åŒ…å›´çš„å†…å®¹
            r'![^!]*!',  # æ„Ÿå¹å·åŒ…å›´çš„å†…å®¹
            r'ç½‘ç«™æ— éšœç¢',  # æ— éšœç¢é“¾æ¥
            r'PCç‰ˆæœ¬',  # ç‰ˆæœ¬åˆ‡æ¢
            r'å®¢æˆ·ç«¯',  # å®¢æˆ·ç«¯ä¸‹è½½
            r'å­—ä½“ï¼š\s*å°\s*ä¸­\s*å¤§',  # å­—ä½“å¤§å°é€‰æ‹©
            r'åˆ†äº«åˆ°ï¼š.*?\)',  # åˆ†äº«æŒ‰é’®
            r'\([^)]*javascript[^)]*\)',  # åŒ…å«javascriptçš„æ‹¬å·å†…å®¹
            r'æ¥æºï¼š[^\n]*\n',  # æ¥æºä¿¡æ¯ï¼ˆä¿ç•™ä½†ä¸é‡å¤ï¼‰
            r'^\d{2}/\d{2}\s+\d{2}:\d{2}:\d{2}',  # æ—¶é—´æˆ³
        ]
        
        cleaned_content = content
        for pattern in unwanted_patterns:
            cleaned_content = re.sub(pattern, '', cleaned_content, flags=re.IGNORECASE | re.DOTALL)
        
        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        cleaned_content = re.sub(r'\n\s*\n', '\n\n', cleaned_content)  # åˆå¹¶å¤šä¸ªç©ºè¡Œ
        cleaned_content = re.sub(r'[ \t]+', ' ', cleaned_content)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
        cleaned_content = cleaned_content.strip()
        
        return cleaned_content
    
    def _extract_clean_content_from_markdown(self, markdown):
        """ä»markdownä¸­æå–å¹²å‡€çš„æ–‡ç« å†…å®¹"""
        if not markdown:
            return ""
        
        # åˆ†æ­¥éª¤æ¸…ç†å†…å®¹
        content = markdown
        
        # 1. ç§»é™¤å¤§å—çš„å¯¼èˆªåŒºåŸŸï¼ˆä½¿ç”¨æ›´ç²¾ç¡®çš„æ¨¡å¼ï¼‰
        navigation_patterns = [
            r'!\[.*?\]\([^)]*\).*?æ‰‹æœºç‰ˆ.*?ç½‘ç«™åœ°å›¾.*?åœ°æ–¹é¢‘é“.*?å¤šè¯­ç§é¢‘é“.*?æ–°åæŠ¥åˆŠ.*?æ‰¿å»ºç½‘ç«™.*?å®¢æˆ·ç«¯',  # å®Œæ•´å¯¼èˆªå—ï¼ˆåŒ…å«å›¾ç‰‡ï¼‰
            r'æ‰‹æœºç‰ˆ.*?ç«™å†…æœç´¢.*?æ–°åé€šè®¯ç¤¾ä¸»åŠ',  # ç§»åŠ¨ç‰ˆå¯¼èˆªåˆ°ä¸»åŠæ–¹
            r'Copyright.*?All Rights Reserved.*?åˆ¶ä½œå•ä½ï¼š.*?ç‰ˆæƒæ‰€æœ‰ï¼š[^\n]*',  # ç‰ˆæƒä¿¡æ¯å—
            r'\[.*?\]\([^)]*\)\s*\*\s*\[.*?\]\([^)]*\)\s*\*.*?åœ°æ–¹é¢‘é“',  # é“¾æ¥åˆ—è¡¨æ¨¡å¼
            r'åœ°æ–¹é¢‘é“\s*\*.*?å¤šè¯­ç§é¢‘é“',  # åœ°æ–¹é¢‘é“åˆ—è¡¨
            r'å¤šè¯­ç§é¢‘é“\s*\*.*?æ–°åæŠ¥åˆŠ',  # å¤šè¯­ç§é¢‘é“åˆ—è¡¨
            r'æ–°åæŠ¥åˆŠ\s*\[.*?\].*?æ‰¿å»ºç½‘ç«™',  # æŠ¥åˆŠåˆ—è¡¨
            r'æ‰¿å»ºç½‘ç«™\s*\[.*?\].*?å®¢æˆ·ç«¯',  # æ‰¿å»ºç½‘ç«™åˆ—è¡¨
        ]
        
        for pattern in navigation_patterns:
            content = re.sub(pattern, '', content, flags=re.IGNORECASE | re.DOTALL)
        
        # 2. ç§»é™¤å°çš„æ— å…³å…ƒç´ 
        small_unwanted_patterns = [
            r'javascript:void\([^)]*\)',  # JavaScripté“¾æ¥
            r'å­—ä½“ï¼š\s*å°\s*ä¸­\s*å¤§',  # å­—ä½“å¤§å°é€‰æ‹©
            r'åˆ†äº«åˆ°ï¼š[^#\n]*',  # åˆ†äº«æŒ‰é’®ï¼ˆä½†ä¿ç•™#æ ‡é¢˜ï¼‰
            r'\([^)]*javascript[^)]*\)',  # åŒ…å«javascriptçš„æ‹¬å·å†…å®¹
            r'ç½‘ç«™æ— éšœç¢',  # æ— éšœç¢é“¾æ¥
            r'PCç‰ˆæœ¬',  # ç‰ˆæœ¬åˆ‡æ¢
            r'!\[.*?\]\([^)]*\)',  # Markdownå›¾ç‰‡é“¾æ¥
            r'https?://[^\s\)]+',  # æ¸…ç†æ®‹ç•™çš„URLï¼ˆä½†ä¸åœ¨æ‹¬å·å†…çš„ï¼‰
            r'\[.*?\]\([^)]*\)\s*\*\s*',  # é“¾æ¥åçš„æ˜Ÿå·
            r'\*\s*\[.*?\]\([^)]*\)',  # æ˜Ÿå·å¼€å¤´çš„é“¾æ¥
            r'^\s*\*\s*.*?$',  # ä»¥æ˜Ÿå·å¼€å¤´çš„è¡Œï¼ˆé€šå¸¸æ˜¯å¯¼èˆªé¡¹ï¼‰
            r'^\s*\[.*?\]\([^)]*\)\s*$',  # å•ç‹¬çš„é“¾æ¥è¡Œ
        ]
        
        for pattern in small_unwanted_patterns:
            content = re.sub(pattern, '', content, flags=re.IGNORECASE)
        
        # 3. æ™ºèƒ½æå–æ–‡ç« ä¸»ä½“
        lines = content.split('\n')
        
        # æ‰¾åˆ°æ–‡ç« æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯#å¼€å¤´æˆ–åŒ…å«å…³é”®è¯ï¼‰
        title_idx = -1
        for i, line in enumerate(lines):
            line = line.strip()
            if line.startswith('#') and ('æ–°åç½‘' in line or 'è¯„' in line or len(line) > 15):
                title_idx = i
                break
        
        # å¦‚æœæ‰¾åˆ°æ ‡é¢˜ï¼Œä»æ ‡é¢˜å¼€å§‹æå–
        if title_idx >= 0:
            start_idx = title_idx
        else:
            # å¦åˆ™æ‰¾åˆ°ç¬¬ä¸€ä¸ªçœ‹èµ·æ¥åƒæ­£æ–‡çš„è¡Œ
            start_idx = 0
            for i, line in enumerate(lines):
                line = line.strip()
                if (line and len(line) > 20 and 
                    not any(keyword in line.lower() for keyword in [
                        'é¦–é¡µ', 'å¯¼èˆª', 'èœå•', 'ç™»å½•', 'æ³¨å†Œ', 'æœç´¢', 'ç½‘ç«™åœ°å›¾'
                    ]) and
                    ('æ–°åç½‘' in line or 'è®°è€…' in line or line.endswith('ç”µ') or 'æ—¥' in line)):
                    start_idx = i
                    break
        
        # æ‰¾åˆ°æ–‡ç« ç»“æŸä½ç½®
        end_idx = len(lines)
        for i in range(start_idx + 1, len(lines)):
            line = lines[i].strip()
            if any(keyword in line.lower() for keyword in [
                'copyright', 'ç‰ˆæƒæ‰€æœ‰', 'åˆ¶ä½œå•ä½', 'è´£ä»»ç¼–è¾‘', 'çº é”™'
            ]):
                end_idx = i
                break
        
        # æå–æ–‡ç« ä¸»ä½“
        article_lines = lines[start_idx:end_idx]
        article_content = '\n'.join(article_lines).strip()
        
        # 4. æœ€åæ¸…ç†
        article_content = re.sub(r'\n{3,}', '\n\n', article_content)  # é™åˆ¶è¿ç»­ç©ºè¡Œ
        article_content = re.sub(r'[ \t]+', ' ', article_content)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
        
        return article_content
    
    def _parse_publish_date(self, date_str, markdown_content="", title=""):
        """è§£ææ–‡ç« å‘å¸ƒæ—¥æœŸ"""
        if not date_str or not date_str.strip():
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„æ—¥æœŸï¼Œå°è¯•ä»å…¶ä»–ä¿¡æ¯ä¸­æå–
            date_str = self._extract_date_from_content(markdown_content + " " + title)
        
        if not date_str or not date_str.strip():
            return None
        
        try:
            # æ¸…ç†æ—¥æœŸå­—ç¬¦ä¸²
            date_str = date_str.strip()
            
            # å¸¸è§çš„ä¸­æ–‡æ—¥æœŸæ ¼å¼ï¼ˆä¼˜å…ˆåŒ¹é…å¸¦æ—¶é—´çš„æ ¼å¼ï¼‰
            chinese_patterns = [
                r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[\s]*(\d{1,2}):(\d{2})',  # 2025å¹´9æœˆ11æ—¥ 08:02
                r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',  # 2025å¹´9æœˆ11æ—¥
                r'(\d{1,2})æœˆ(\d{1,2})æ—¥[\s]*(\d{1,2}):(\d{2})',  # 9æœˆ11æ—¥ 08:02
                r'(\d{1,2})æœˆ(\d{1,2})æ—¥',  # 9æœˆ11æ—¥
                r'ä»Šå¤©[\s]*(\d{1,2}):(\d{2})',  # ä»Šå¤© 08:02
                r'æ˜¨å¤©[\s]*(\d{1,2}):(\d{2})',  # æ˜¨å¤© 08:02
                r'å‰å¤©[\s]*(\d{1,2}):(\d{2})',  # å‰å¤© 08:02
                r'ä»Šå¤©',
                r'æ˜¨å¤©',
                r'å‰å¤©',
                r'(\d+)å°æ—¶å‰',
                r'(\d+)åˆ†é’Ÿå‰',
                r'åˆšåˆš'
            ]
            
            # å¤„ç†ä¸­æ–‡æ—¥æœŸ
            for pattern in chinese_patterns:
                match = re.search(pattern, date_str)
                if match:
                    return self._parse_chinese_date(match, pattern, date_str)
            
            # å°è¯•å¤„ç†æ ‡å‡†æ ¼å¼ï¼ˆä¿ç•™ç©ºæ ¼ï¼Œå› ä¸ºéœ€è¦åˆ†ç¦»æ—¥æœŸå’Œæ—¶é—´ï¼‰
            # å…ˆå¤„ç†å¸¦æ—¶é—´çš„æ ¼å¼ï¼ˆåŒ…å«ç©ºæ ¼ï¼‰
            datetime_match = re.search(r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})[\s]+(\d{1,2}:\d{2})', date_str)
            if datetime_match:
                date_part, time_part = datetime_match.groups()
                try:
                    # è§£ææ—¥æœŸéƒ¨åˆ†
                    if '-' in date_part:
                        year, month, day = map(int, date_part.split('-'))
                    else:
                        year, month, day = map(int, date_part.split('/'))
                    
                    # è§£ææ—¶é—´éƒ¨åˆ†
                    hour, minute = map(int, time_part.split(':'))
                    return datetime(year, month, day, hour, minute, 0)
                except ValueError:
                    pass
            
            # å¤„ç†ä¸å¸¦æ—¶é—´çš„æ ‡å‡†æ ¼å¼
            clean_date = re.sub(r'[^\d\-/]', '', date_str)
            
            if re.match(r'\d{4}-\d{1,2}-\d{1,2}$', clean_date):
                # æ ‡å‡†æ ¼å¼ YYYY-MM-DD
                parts = clean_date.split('-')
                year, month, day = int(parts[0]), int(parts[1]), int(parts[2])
                return datetime(year, month, day, 12, 0, 0)
            elif re.match(r'\d{4}/\d{1,2}/\d{1,2}$', clean_date):
                # æ ¼å¼ YYYY/MM/DD
                parts = clean_date.split('/')
                year, month, day = int(parts[0]), int(parts[1]), int(parts[2])
                return datetime(year, month, day, 12, 0, 0)
            
            # ä½¿ç”¨dateutilè§£æå…¶ä»–æ ‡å‡†æ ¼å¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if DATEUTIL_AVAILABLE:
                try:
                    parsed_date = date_parser.parse(date_str, fuzzy=True)
                    
                    # å¦‚æœè§£æå‡ºçš„æ—¥æœŸæ˜¯æœªæ¥æ—¥æœŸï¼Œå¯èƒ½æœ‰è¯¯ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
                    if parsed_date > datetime.now():
                        return datetime.now()
                    
                    return parsed_date
                except:
                    pass
            
            # ä½¿ç”¨æ ‡å‡†åº“è§£æå¸¸è§è‹±æ–‡æ ¼å¼
            english_formats = [
                '%b %d, %Y',      # Sep 11, 2025
                '%B %d, %Y',      # September 11, 2025
                '%Y-%m-%d %H:%M:%S',  # 2025-09-11 15:30:00
                '%Y-%m-%d %H:%M',     # 2025-09-11 15:30
                '%m/%d/%Y',       # 09/11/2025
                '%d/%m/%Y',       # 11/09/2025
            ]
            
            for fmt in english_formats:
                try:
                    parsed_date = datetime.strptime(date_str, fmt)
                    if parsed_date > datetime.now():
                        return datetime.now()
                    return parsed_date
                except ValueError:
                    continue
            
            # å¦‚æœéƒ½æ— æ³•è§£æï¼Œè¿”å›None
            return None
            
        except Exception as e:
            logger.debug(f"æ—¥æœŸè§£æå¤±è´¥: {date_str}, é”™è¯¯: {e}")
            return None
    
    def _extract_date_from_content(self, content):
        """ä»å†…å®¹ä¸­æå–æ—¥æœŸä¿¡æ¯"""
        if not content:
            return None
        
        # å¸¸è§çš„æ—¥æœŸæ¨¡å¼ï¼ˆåŒ…å«æ—¶é—´ï¼‰
        date_patterns = [
            r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥[\s]*\d{1,2}:\d{2})',  # 2025å¹´9æœˆ11æ—¥ 08:02
            r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',  # 2025å¹´9æœˆ11æ—¥
            r'(\d{4}-\d{1,2}-\d{1,2}[\s]+\d{1,2}:\d{2}:\d{2})',  # 2025-09-11 08:02:30
            r'(\d{4}-\d{1,2}-\d{1,2}[\s]+\d{1,2}:\d{2})',  # 2025-09-11 08:02
            r'(\d{4}-\d{1,2}-\d{1,2})',  # 2025-09-11
            r'(\d{1,2}æœˆ\d{1,2}æ—¥[\s]*\d{1,2}:\d{2})',  # 9æœˆ11æ—¥ 08:02
            r'(\d{1,2}æœˆ\d{1,2}æ—¥)',  # 9æœˆ11æ—¥
            r'(\d+å°æ—¶å‰)',
            r'(\d+åˆ†é’Ÿå‰)',
            r'(ä»Šå¤©|æ˜¨å¤©|å‰å¤©)',
            r'(åˆšåˆš)',
            r'å‘å¸ƒäº[\s]*(\d{4}-\d{1,2}-\d{1,2}[\s]*\d{1,2}:\d{2})',
            r'æ—¶é—´[:ï¼š][\s]*(\d{4}-\d{1,2}-\d{1,2}[\s]*\d{1,2}:\d{2})',
            r'(\d{4}/\d{1,2}/\d{1,2}[\s]+\d{1,2}:\d{2})',  # 2025/09/11 08:02
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, content)
            if match:
                return match.group(1)
        
        return None
    
    def _parse_chinese_date(self, match, pattern, original_str):
        """è§£æä¸­æ–‡æ—¥æœŸæ ¼å¼"""
        try:
            now = datetime.now()
            groups = match.groups()
            
            # å¸¦æ—¶é—´çš„ä»Šå¤©/æ˜¨å¤©/å‰å¤©
            if 'ä»Šå¤©' in original_str and len(groups) >= 2:
                hour, minute = int(groups[-2]), int(groups[-1])
                return now.replace(hour=hour, minute=minute, second=0, microsecond=0)
            elif 'æ˜¨å¤©' in original_str and len(groups) >= 2:
                hour, minute = int(groups[-2]), int(groups[-1])
                return (now - timedelta(days=1)).replace(hour=hour, minute=minute, second=0, microsecond=0)
            elif 'å‰å¤©' in original_str and len(groups) >= 2:
                hour, minute = int(groups[-2]), int(groups[-1])
                return (now - timedelta(days=2)).replace(hour=hour, minute=minute, second=0, microsecond=0)
            # ä¸å¸¦æ—¶é—´çš„ä»Šå¤©/æ˜¨å¤©/å‰å¤©
            elif 'ä»Šå¤©' in original_str:
                return now.replace(hour=12, minute=0, second=0, microsecond=0)
            elif 'æ˜¨å¤©' in original_str:
                return (now - timedelta(days=1)).replace(hour=12, minute=0, second=0, microsecond=0)
            elif 'å‰å¤©' in original_str:
                return (now - timedelta(days=2)).replace(hour=12, minute=0, second=0, microsecond=0)
            elif 'å°æ—¶å‰' in original_str:
                hours = int(match.group(1))
                return now - timedelta(hours=hours)
            elif 'åˆ†é’Ÿå‰' in original_str:
                minutes = int(match.group(1))
                return now - timedelta(minutes=minutes)
            elif 'åˆšåˆš' in original_str:
                return now - timedelta(minutes=5)
            # å¸¦æ—¶é—´çš„å®Œæ•´æ—¥æœŸï¼š2025å¹´9æœˆ11æ—¥ 08:02
            elif 'å¹´' in pattern and 'æœˆ' in pattern and 'æ—¥' in pattern and len(groups) >= 5:
                year, month, day, hour, minute = int(groups[0]), int(groups[1]), int(groups[2]), int(groups[3]), int(groups[4])
                return datetime(year, month, day, hour, minute, 0)
            # å¸¦æ—¶é—´çš„æœˆæ—¥ï¼š9æœˆ11æ—¥ 08:02
            elif 'æœˆ' in pattern and 'æ—¥' in pattern and len(groups) >= 4:
                month, day, hour, minute = int(groups[0]), int(groups[1]), int(groups[2]), int(groups[3])
                return datetime(now.year, month, day, hour, minute, 0)
            # ä¸å¸¦æ—¶é—´çš„å®Œæ•´æ—¥æœŸï¼š2025å¹´9æœˆ11æ—¥
            elif 'å¹´' in pattern and 'æœˆ' in pattern and 'æ—¥' in pattern:
                year = int(match.group(1))
                month = int(match.group(2))
                day = int(match.group(3))
                return datetime(year, month, day, 12, 0, 0)
            # ä¸å¸¦æ—¶é—´çš„æœˆæ—¥ï¼š9æœˆ11æ—¥
            elif 'æœˆ' in pattern and 'æ—¥' in pattern:
                month = int(match.group(1))
                day = int(match.group(2))
                return datetime(now.year, month, day, 12, 0, 0)
            
        except Exception as e:
            logger.debug(f"ä¸­æ–‡æ—¥æœŸè§£æå¤±è´¥: {original_str}, é”™è¯¯: {e}")
        
        return None
    
    async def run_crawler_task(self, crawler_config):
        """æ‰§è¡Œçˆ¬è™«ä»»åŠ¡"""
        try:
            # å¯¼å…¥æ•°æ®åº“ç›¸å…³æ¨¡å—ï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼‰
            from models import db
            
            logger.info(f"å¼€å§‹æ‰§è¡Œçˆ¬è™«ä»»åŠ¡: {crawler_config.name}")
            
            # 1. ä»åˆ—è¡¨é¡µæå–URL
            urls = await self.extract_urls_from_page(
                crawler_config.list_url, 
                crawler_config.url_regex
            )
            
            if not urls:
                logger.warning(f"çˆ¬è™« {crawler_config.name} æœªæ‰¾åˆ°ä»»ä½•URL")
                
                # å³ä½¿æ²¡æœ‰æ‰¾åˆ°URLä¹Ÿè¦æ›´æ–°æœ€åè¿è¡Œæ—¶é—´
                try:
                    crawler_config.last_run = datetime.utcnow()
                    db.session.commit()
                    logger.info(f"âœ… å·²æ›´æ–°çˆ¬è™« {crawler_config.name} çš„æœ€åè¿è¡Œæ—¶é—´ï¼ˆæœªæ‰¾åˆ°URLï¼‰")
                except Exception as e:
                    logger.error(f"æ›´æ–°last_runæ—¶é—´å¤±è´¥: {e}")
                    db.session.rollback()
                
                return {
                    'success': True,
                    'saved_count': 0,
                    'failed_count': 0,
                    'total_processed': 0
                }
            
            logger.info(f"çˆ¬è™« {crawler_config.name} æ‰¾åˆ° {len(urls)} ä¸ªURL")
            
            # 2. è¿‡æ»¤å·²å­˜åœ¨çš„URLï¼Œé¿å…é‡å¤çˆ¬å–
            new_urls = await self._filter_new_urls(urls, crawler_config.id)
            
            if not new_urls:
                logger.info(f"çˆ¬è™« {crawler_config.name} æ‰€æœ‰URLéƒ½å·²å­˜åœ¨ï¼Œè·³è¿‡çˆ¬å–")
                
                # å³ä½¿æ²¡æœ‰æ–°URLä¹Ÿè¦æ›´æ–°æœ€åè¿è¡Œæ—¶é—´
                try:
                    crawler_config.last_run = datetime.utcnow()
                    db.session.commit()
                    logger.info(f"âœ… å·²æ›´æ–°çˆ¬è™« {crawler_config.name} çš„æœ€åè¿è¡Œæ—¶é—´ï¼ˆæ— æ–°URLï¼‰")
                except Exception as e:
                    logger.error(f"æ›´æ–°last_runæ—¶é—´å¤±è´¥: {e}")
                    db.session.rollback()
                
                return {
                    'success': True,
                    'saved_count': 0,
                    'failed_count': 0,
                    'total_processed': 0
                }
            
            logger.info(f"çˆ¬è™« {crawler_config.name} è¿‡æ»¤åéœ€è¦çˆ¬å– {len(new_urls)} ä¸ªæ–°URL")
            
            # 3. é€ä¸ªæŠ“å–æ–‡ç« å†…å®¹å¹¶ç«‹å³ä¿å­˜ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            saved_count = 0
            failed_count = 0
            base_delay = 1.0  # åŸºç¡€å»¶è¿Ÿ
            
            for i, url in enumerate(new_urls[:20]):  # é™åˆ¶æ¯æ¬¡æœ€å¤šæŠ“å–20ç¯‡æ–‡ç« 
                logger.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {i+1}/{min(len(new_urls), 20)} ç¯‡æ–‡ç« ")
                
                # å¸¦é‡è¯•çš„æŠ“å–
                result = await self._crawl_with_retry(url, max_retries=3)
                
                # ç«‹å³ä¿å­˜åˆ°æ•°æ®åº“
                save_success = await self._save_single_result(result, crawler_config)
                if save_success:
                    saved_count += 1
                    # æˆåŠŸæ—¶ä½¿ç”¨åŸºç¡€å»¶è¿Ÿ
                    delay = base_delay
                else:
                    failed_count += 1
                    # å¤±è´¥æ—¶å¢åŠ å»¶è¿Ÿ
                    delay = base_delay * 2
                
                # åŠ¨æ€å»¶è¿Ÿé¿å…è¢«å°
                logger.debug(f"ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...")
                await asyncio.sleep(delay)
            
            logger.info(f"çˆ¬è™« {crawler_config.name} å®Œæˆï¼ŒæˆåŠŸä¿å­˜ {saved_count} ç¯‡æ–‡ç« ï¼Œå¤±è´¥ {failed_count} ç¯‡")
            
            # æ›´æ–°çˆ¬è™«æœ€åè¿è¡Œæ—¶é—´
            try:
                crawler_config.last_run = datetime.utcnow()
                db.session.commit()
                logger.info(f"âœ… å·²æ›´æ–°çˆ¬è™« {crawler_config.name} çš„æœ€åè¿è¡Œæ—¶é—´")
            except Exception as e:
                logger.error(f"æ›´æ–°last_runæ—¶é—´å¤±è´¥: {e}")
                db.session.rollback()
            
            # è¿”å›ç»Ÿè®¡ä¿¡æ¯è€Œä¸æ˜¯å…·ä½“ç»“æœ
            return {
                'success': True,
                'saved_count': saved_count,
                'failed_count': failed_count,
                'total_processed': saved_count + failed_count
            }
        
        except Exception as e:
            logger.error(f"æ‰§è¡Œçˆ¬è™«ä»»åŠ¡å¤±è´¥ {crawler_config.name}: {e}")
            
            # å³ä½¿å¤±è´¥ä¹Ÿè¦æ›´æ–°æœ€åè¿è¡Œæ—¶é—´
            try:
                crawler_config.last_run = datetime.utcnow()
                db.session.commit()
                logger.info(f"âœ… å·²æ›´æ–°çˆ¬è™« {crawler_config.name} çš„æœ€åè¿è¡Œæ—¶é—´ï¼ˆä»»åŠ¡å¤±è´¥ï¼‰")
            except Exception as update_error:
                logger.error(f"æ›´æ–°last_runæ—¶é—´å¤±è´¥: {update_error}")
                db.session.rollback()
            
            return []
    
    async def _crawl_with_retry(self, url, max_retries=3):
        """å¸¦é‡è¯•æœºåˆ¶çš„æ–‡ç« æŠ“å–"""
        last_error = None
        
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    # é‡è¯•æ—¶å¢åŠ å»¶è¿Ÿï¼ˆæŒ‡æ•°é€€é¿ï¼‰
                    retry_delay = (2 ** attempt) * 1.0  # 1ç§’, 2ç§’, 4ç§’
                    logger.info(f"ç¬¬ {attempt + 1} æ¬¡é‡è¯• {url}ï¼Œç­‰å¾… {retry_delay:.1f} ç§’...")
                    await asyncio.sleep(retry_delay)
                
                result = await self.crawl_article_content(url)
                
                # å¦‚æœæˆåŠŸï¼Œç›´æ¥è¿”å›
                if result['success']:
                    if attempt > 0:
                        logger.info(f"âœ… é‡è¯•æˆåŠŸ: {url}")
                    return result
                else:
                    # å¦‚æœå¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶ç»§ç»­é‡è¯•
                    last_error = result.get('error', 'Unknown error')
                    logger.warning(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {last_error}")
                    
            except Exception as e:
                last_error = str(e)
                logger.warning(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¼‚å¸¸: {e}")
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        logger.error(f"âŒ {max_retries} æ¬¡é‡è¯•åä»ç„¶å¤±è´¥: {url}, æœ€åé”™è¯¯: {last_error}")
        return {
            'success': False,
            'url': url,
            'error': f"é‡è¯• {max_retries} æ¬¡åå¤±è´¥: {last_error}"
        }
    
    async def _save_single_result(self, result, crawler_config):
        """ç«‹å³ä¿å­˜å•æ¡çˆ¬å–ç»“æœåˆ°æ•°æ®åº“"""
        try:
            # å¯¼å…¥æ¨¡å‹ï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼‰
            from models import CrawlRecord, db
            from app import app
            from datetime import datetime
            
            with app.app_context():
                # å†æ¬¡æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨ï¼ˆé˜²æ­¢å¹¶å‘é—®é¢˜ï¼‰
                existing = CrawlRecord.query.filter_by(
                    url=result['url'], 
                    status='success'
                ).first()
                
                if existing:
                    logger.info(f"URLå·²å­˜åœ¨ï¼Œè·³è¿‡ä¿å­˜: {result['url']}")
                    return True  # ç®—ä½œæˆåŠŸï¼Œå› ä¸ºæ•°æ®å·²å­˜åœ¨
                
                if result['success']:
                    # è§£æå‘å¸ƒæ—¥æœŸ
                    publish_date = self._parse_publish_date(
                        result.get('date', ''),
                        result.get('markdown', ''),
                        result.get('title', '')
                    )
                    
                    record = CrawlRecord(
                        crawler_config_id=crawler_config.id,
                        url=result['url'],
                        title=result.get('title', ''),
                        content=result.get('content', ''),
                        author=result.get('author', ''),
                        publish_date=publish_date,  # ä½¿ç”¨è§£æå‡ºçš„å‘å¸ƒæ—¥æœŸ
                        crawled_at=datetime.utcnow(),  # çˆ¬å–æ—¶é—´
                        status='success'
                    )
                    
                    date_info = publish_date.strftime('%Y-%m-%d %H:%M') if publish_date else 'æœªçŸ¥'
                    logger.info(f"âœ… ä¿å­˜æˆåŠŸè®°å½• [å‘å¸ƒ:{date_info}]: {result.get('title', result['url'])[:50]}...")
                else:
                    record = CrawlRecord(
                        crawler_config_id=crawler_config.id,
                        url=result['url'],
                        crawled_at=datetime.utcnow(),
                        status='failed',
                        error_message=result.get('error', '')
                    )
                    logger.info(f"âŒ ä¿å­˜å¤±è´¥è®°å½•: {result['url'][:60]}... - {result.get('error', 'Unknown error')}")
                
                db.session.add(record)
                
                # ç«‹å³æäº¤è¿™æ¡è®°å½•
                db.session.commit()
                logger.debug(f"ğŸ’¾ å•æ¡è®°å½•å·²æäº¤åˆ°æ•°æ®åº“")
                
                return True
                
        except Exception as e:
            logger.error(f"ä¿å­˜å•æ¡è®°å½•å¤±è´¥: {e}, URL: {result.get('url', 'Unknown')}")
            # å›æ»šè¿™æ¬¡æ“ä½œ
            try:
                from app import app
                with app.app_context():
                    db.session.rollback()
            except:
                pass
            return False
    
    async def _filter_new_urls(self, urls, crawler_config_id):
        """è¿‡æ»¤å‡ºæ–°çš„URLï¼Œé¿å…é‡å¤çˆ¬å–"""
        try:
            # å¯¼å…¥CrawlRecordæ¨¡å‹ï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼‰
            from models import CrawlRecord
            
            new_urls = []
            for url in urls:
                # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“ä¸­
                existing = CrawlRecord.query.filter_by(
                    url=url,
                    status='success'
                ).first()
                
                if not existing:
                    new_urls.append(url)
                else:
                    logger.debug(f"URLå·²å­˜åœ¨ï¼Œè·³è¿‡: {url}")
            
            logger.info(f"URLè¿‡æ»¤å®Œæˆ: æ€»å…± {len(urls)} ä¸ªï¼Œæ–°å¢ {len(new_urls)} ä¸ªï¼Œå·²å­˜åœ¨ {len(urls) - len(new_urls)} ä¸ª")
            return new_urls
            
        except Exception as e:
            logger.error(f"è¿‡æ»¤URLå¤±è´¥: {e}")
            # å¦‚æœè¿‡æ»¤å¤±è´¥ï¼Œè¿”å›åŸå§‹URLåˆ—è¡¨ï¼Œç¡®ä¿çˆ¬è™«èƒ½ç»§ç»­å·¥ä½œ
            return urls
    
    def generate_regex_with_ai(self, page_content, sample_titles):
        """ä½¿ç”¨AIç”Ÿæˆæ­£åˆ™è¡¨è¾¾å¼ï¼ˆå ä½ç¬¦å®ç°ï¼‰"""
        # è¿™é‡Œå¯ä»¥é›†æˆLLMæ¥æ™ºèƒ½ç”Ÿæˆæ­£åˆ™è¡¨è¾¾å¼
        # ç›®å‰æä¾›ä¸€ä¸ªç®€å•çš„å®ç°
        
        # åŸºäºæ ·æœ¬æ ‡é¢˜ç”Ÿæˆç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼
        if not sample_titles:
            return r'([^"]*\.html?[^"]*)"'
        
        # åˆ†ææ ·æœ¬æ ‡é¢˜ï¼Œå°è¯•æ‰¾åˆ°å…±åŒæ¨¡å¼
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œå®é™…å¯ä»¥ä½¿ç”¨LLMæ¥ç”Ÿæˆæ›´æ™ºèƒ½çš„æ­£åˆ™
        
        # å¸¸è§çš„æ–‡ç« é“¾æ¥æ¨¡å¼
        patterns = [
            r'([^"]*article[^"]*)"',
            r'([^"]*news[^"]*)"',
            r'([^"]*post[^"]*)"',
            r'([^"]*\.html[^"]*)"',
            r'"(/[^"]*\d{4}[^"]*)"'  # åŒ…å«å¹´ä»½çš„é“¾æ¥
        ]
        
        # è¿”å›ç¬¬ä¸€ä¸ªæ¨¡å¼ä½œä¸ºé»˜è®¤
        return patterns[0]
